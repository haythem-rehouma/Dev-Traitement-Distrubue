 # Exemples Spark en Scala

### **1Ô∏è‚É£ Chargement d'un CSV en DataFrame**
```scala
import org.apache.spark.sql.{SparkSession, DataFrame}

val spark = SparkSession.builder().appName("Load CSV").master("local[*]").getOrCreate()
val df: DataFrame = spark.read.option("header", "true").csv("file:///C:/Users/rehou/Downloads/AAPL.csv")
df.show()
```

---

### **2Ô∏è‚É£ Ajouter une colonne calcul√©e (TVA 20%)**
```scala
import org.apache.spark.sql.functions._

val dfAug = df.withColumn("prix_ttc", col("closeprice") * 1.20)
dfAug.show()
```

---

### **3Ô∏è‚É£ Filtrage des donn√©es**
```scala
val dfFiltered = df.filter(col("closeprice") > 100)
dfFiltered.show()
```

---

### **4Ô∏è‚É£ Agr√©gation (Moyenne des prix)**
```scala
df.groupBy("dt").agg(avg("closeprice").as("moyenne_close")).show()
```

---

### **5Ô∏è‚É£ Pivot Table**
```scala
df.groupBy("dt").pivot("openprice").agg(sum("closeprice")).show()
```

---

### **6Ô∏è‚É£ Jointure entre deux DataFrames**
```scala
val df1 = df.select("dt", "closeprice")
val df2 = df.select("dt", "volume")
val joinedDF = df1.join(df2, Seq("dt"))
joinedDF.show()
```

---

### **7Ô∏è‚É£ Tri des donn√©es**
```scala
df.orderBy(desc("closeprice")).show()
```

---

### **8Ô∏è‚É£ UDF (User-Defined Function)**
```scala
import org.apache.spark.sql.functions.udf

val categorize = udf((price: Double) => if (price > 100) "High" else "Low")
val dfUDF = df.withColumn("category", categorize(col("closeprice")))
dfUDF.show()
```

---

### **9Ô∏è‚É£ Cr√©ation d‚Äôune Table Temporaire & Requ√™te SQL**
```scala
df.createOrReplaceTempView("stocks")
val result = spark.sql("SELECT dt, closeprice FROM stocks WHERE closeprice > 100")
result.show()
```

---

### **üîü Streaming (Lecture en direct d‚Äôun Socket)**
```scala
val streamingDF = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).load()
val wordCounts = streamingDF.groupBy("value").count()
wordCounts.writeStream.outputMode("complete").format("console").start().awaitTermination()
```

---

### **1Ô∏è‚É£1Ô∏è‚É£ √âcriture en JSON**
```scala
df.write.mode("overwrite").json("file:///C:/Users/rehou/Downloads/output_json")
```

---

### **1Ô∏è‚É£2Ô∏è‚É£ √âcriture en Parquet**
```scala
df.write.mode("overwrite").parquet("file:///C:/Users/rehou/Downloads/output_parquet")
```

---

### **1Ô∏è‚É£3Ô∏è‚É£ √âcriture en CSV**
```scala
df.write.mode("overwrite").csv("file:///C:/Users/rehou/Downloads/output_csv")
```

---

### **1Ô∏è‚É£4Ô∏è‚É£ Calcul d‚Äôune Moyenne Mobile**
```scala
import org.apache.spark.sql.expressions.Window
val windowSpec = Window.orderBy("dt").rowsBetween(-4, 0)
val dfMovingAvg = df.withColumn("moving_avg", avg("closeprice").over(windowSpec))
dfMovingAvg.show()
```

---

### **1Ô∏è‚É£5Ô∏è‚É£ Accumulateurs (Compter les √©l√©ments filtr√©s)**
```scala
val acc = spark.sparkContext.longAccumulator("countFiltered")
val filteredRDD = df.rdd.filter(row => { if (row.getAs[Double]("closeprice") < 100) { acc.add(1); false } else true })
println(s"Nombre d'√©l√©ments filtr√©s: ${acc.value}")
```

---

### **1Ô∏è‚É£6Ô∏è‚É£ Variables Broadcast**
```scala
val broadcastVar = spark.sparkContext.broadcast(Map("AAPL" -> "Apple", "GOOGL" -> "Google"))
val dfWithCompany = df.withColumn("company_name", lit(broadcastVar.value.getOrElse("AAPL", "Unknown")))
dfWithCompany.show()
```

---

### **1Ô∏è‚É£7Ô∏è‚É£ Partitionner les donn√©es**
```scala
val dfPartitioned = df.repartition(5)
println(s"Nombre de partitions: ${dfPartitioned.rdd.getNumPartitions}")
```

---

### **1Ô∏è‚É£8Ô∏è‚É£ Utilisation de `mapPartitions`**
```scala
val rdd = spark.sparkContext.parallelize(1 to 100, 4)
val processedRdd = rdd.mapPartitions(iter => Iterator(iter.sum))
processedRdd.collect().foreach(println)
```

---

### **1Ô∏è‚É£9Ô∏è‚É£ Utilisation de `mapPartitionsWithIndex`**
```scala
val indexedRDD = df.rdd.mapPartitionsWithIndex((index, iter) => iter.map(row => s"Partition: $index -> $row"))
indexedRDD.collect().foreach(println)
```

---

### **2Ô∏è‚É£0Ô∏è‚É£ Checkpointing**
```scala
spark.sparkContext.setCheckpointDir("file:///C:/Users/rehou/Downloads/checkpoints")
val rdd = spark.sparkContext.parallelize(1 to 100000).map(_ * 2)
rdd.checkpoint()
rdd.count()
```

---
# Annexes  id√©es de projets
---



### 1Ô∏è‚É£ **Projet 1 : Analyse des stocks boursiers**
- **Objectif** : Charger un fichier CSV contenant des donn√©es boursi√®res, le convertir en `RDD`, puis en `DataFrame` et calculer des indicateurs financiers.
- **T√¢ches** :
  1. Lire et parser un fichier CSV (exclure l'en-t√™te).
  2. Transformer les donn√©es en `RDD[Stock]` puis en `DataFrame`.
  3. Calculer la **moyenne mobile** sur 5 jours.
  4. Partitionner et sauvegarder les r√©sultats en **Parquet**.
  5. G√©n√©rer des statistiques : prix moyen, volume moyen.

### 2Ô∏è‚É£ **Projet 2 : Analyse des logs web**
- **Objectif** : Traiter un fichier de logs Apache (`access.log`), analyser le trafic et d√©tecter les IPs les plus actives.
- **T√¢ches** :
  1. Charger les logs depuis un fichier.
  2. Extraire les champs utiles : IP, URL, code HTTP.
  3. D√©terminer les **top 10 IPs** en termes de requ√™tes.
  4. D√©tecter les erreurs 404 et 500.
  5. Sauvegarder les r√©sultats sous format **Parquet**.

### 3Ô∏è‚É£ **Projet 3 : Recommandation de films avec Spark MLlib**
- **Objectif** : Utiliser un dataset de films (`ratings.csv`) et appliquer un **filtrage collaboratif** pour la recommandation.
- **T√¢ches** :
  1. Charger et nettoyer les donn√©es (`userId, movieId, rating`).
  2. Utiliser l‚Äôalgorithme **ALS (Alternating Least Squares)** pour recommander des films.
  3. √âvaluer le mod√®le en calculant **RMSE**.
  4. Sauvegarder les r√©sultats en **Parquet**.

### 4Ô∏è‚É£ **Projet 4 : D√©tection d‚Äôanomalies sur des transactions**
- **Objectif** : Identifier des transactions suspectes dans un dataset bancaire.
- **T√¢ches** :
  1. Charger un fichier **transactions.csv** (`userId, amount, timestamp`).
  2. D√©terminer les transactions dont le montant d√©passe de **3 √©carts-types** la moyenne de l‚Äôutilisateur.
  3. Utiliser une **fen√™tre temporelle** pour suivre l‚Äô√©volution des d√©penses.
  4. Sauvegarder les r√©sultats sous **format Parquet**.

### üìå **Mat√©riel et outils**
- **Scala + Apache Spark** (`RDD`, `DataFrame`, `Window functions`, `MLlib`)
- **Formats de sortie** : CSV, JSON, Parquet
- **IDE recommand√©s** : IntelliJ, VS Code avec Metals

üí° **Bonus** : Ajoute une partie **optimisation** o√π ils doivent exp√©rimenter avec `cache()`, `persist()`, et la gestion des partitions.

