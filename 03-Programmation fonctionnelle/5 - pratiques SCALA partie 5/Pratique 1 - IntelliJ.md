### **Tutoriel : Ex√©cution d‚Äôun Programme Spark en Scala avec Maven et IntelliJ IDEA**




# Table des Mati√®res : Tutoriel Spark en Scala avec Maven et IntelliJ IDEA

### **1. [Pr√©requis](#1-pr√©requis)**  
- Installation et configuration des outils n√©cessaires.  

### **2. [Cr√©ation du Projet Maven dans IntelliJ IDEA](#2-cr√©ation-du-projet-maven-dans-intellij-idea)**  
- **[Cr√©er un projet Maven](#21-cr√©er-un-projet-maven)**  
- **[Configuration du fichier `pom.xml`](#22-configuration-du-fichier-pomxml)**  
- **[Recharger le projet Maven](#23-recharger-le-projet-maven)**  

### **3. [Ajout du Code Scala](#3-ajout-du-code-scala)**  
- **[Cr√©ation de la classe `StockProcessor.scala`](#31-cr√©ation-de-la-classe-stockprocessorscala)**  
- **[Gestion des fichiers de donn√©es avec RDD et DataFrames](#32-gestion-des-fichiers-de-donn√©es-avec-rdd-et-dataframes)**  

### **4. [Configuration de l‚ÄôEx√©cution](#4-configuration-de-lex√©cution)**  
- Param√©trage des options d‚Äôex√©cution dans IntelliJ IDEA.  

### **5. [Ex√©cution du Programme](#5-ex√©cution-du-programme)**  
- Lancement de l‚Äôapplication Spark.  

### **6. [R√©sultat Attendu](#6-r√©sultat-attendu)**  
- Affichage du tableau de donn√©es trait√©es.  

### **7. [Exercice](#7-exercice)**  
- Manipulation des donn√©es et filtrage.  



## **üìÇ Annexes**
### **üîπ [Annexe 1 : Remarques Importantes](#annexe-1--remarques-importantes)**  
- Versions de Maven, Java et IntelliJ IDEA.  

### **üîπ [Annexe 2 : Arborescence du `pom.xml`](#annexe-2--arborescence-du-pomxml)**  
- Structure d√©taill√©e du fichier `pom.xml`.  

### **üîπ [Annexe 3 : Explication d√©taill√©e du fichier `pom.xml`](#annexe-3--explication-d√©taill√©e-du-fichier-pomxml)**  
- Fonctionnement du `pom.xml` et de ses d√©pendances.  

### **üîπ [Annexe 4 : Explication d√©taill√©e du Code Scala](#annexe-4--explication-d√©taill√©e-du-code-scala)**  
- Description des fonctions et des transformations Spark.  

### **üîπ [Annexe 5 : Comparaison `parseStock` vs `parseRDD`](#annexe-5--comparaison-parsestock-vs-parserdd)**  
- Justification de l‚Äôutilisation des RDD avant conversion en DataFrame.  

### **üîπ [Annexe 6 : Workflow du Programme Scala avec Spark](#annexe-6--workflow-du-programme-scala-avec-spark)**  
- Diagramme et explication du processus de traitement des donn√©es.  

---

### üìå **Liens Rapides**
- üîπ [Retour en haut üîù](#üìñ-table-des-mati√®res--tutoriel-spark-en-scala-avec-maven-et-intellij-idea)  
- üîπ [Aller directement √† l‚Äôex√©cution ‚ñ∂](#5-ex√©cution-du-programme)  



**Objectif** : 

- Apprendre √† configurer un projet Scala avec Maven dans IntelliJ IDEA et ex√©cuter un programme Spark.

---

# **1. Pr√©requis**
Avant de commencer, assurez-vous d'avoir install√© :
- **Apache Spark 3.3.0**  
- **Java 8 (JDK 1.8)** (pour compatibilit√© avec Spark)  
- **Scala 2.12.7** (correspond √† votre version de Spark)  
- **Maven 3.9.0** (version pr√©cis√©e)  
- **IntelliJ IDEA avec le plugin Scala**  

---

# **üìÇ 2. Cr√©ation du Projet Maven dans IntelliJ IDEA**
### **1Ô∏è‚É£ Cr√©er un projet Maven**
1. **Ouvrez IntelliJ IDEA** et s√©lectionnez **"New Project"**.
2. Dans **"Project SDK"**, choisissez **JDK 1.8**.
3. S√©lectionnez **"Maven"** comme type de projet.
4. **D√©cochez** "Create from Archetype".
5. Cliquez sur **Next**, donnez un **nom au projet** et cliquez sur **Finish**.

---

### **2Ô∏è‚É£ Configuration du fichier `pom.xml`**
Dans IntelliJ IDEA, ouvrez le fichier **`pom.xml`** et **remplacez son contenu** par ce code :

```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>sample</groupId>
  <artifactId>scala-module-dependency-sample</artifactId>
  <version>1.0-SNAPSHOT</version>
  <properties>
    <encoding>UTF-8</encoding>
  </properties>
  <!-- Maven profiles allow you to support both Scala 2.10, 2.11 and Scala 2.12 with
    the right dependencies for modules specified for each version separately -->
  <profiles>
    <profile>
      <id>scala-2.12</id>
      <activation>
        <activeByDefault>true</activeByDefault>
      </activation>
      <properties>
        <scala.version>2.12.7</scala.version>
        <scala.compat.version>2.12</scala.compat.version>
      </properties>
      <dependencies>
        <dependency>
          <groupId>org.scala-lang</groupId>
          <artifactId>scala-library</artifactId>
          <version>${scala.version}</version>
        </dependency>
        <dependency>
          <groupId>org.scala-lang.modules</groupId>
          <artifactId>scala-xml_${scala.compat.version}</artifactId>
          <version>1.1.1</version>
        </dependency>
        <dependency>
          <groupId>org.scala-lang.modules</groupId>
          <artifactId>scala-parser-combinators_${scala.compat.version}</artifactId>
          <version>1.1.1</version>
        </dependency>
        <dependency>
          <groupId>org.scala-lang.modules</groupId>
          <artifactId>scala-swing_${scala.compat.version}</artifactId>
          <version>2.0.3</version>
        </dependency>




        <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-core_2.12</artifactId>
          <version>3.3.0</version> <!-- V√©rifiez la compatibilit√© avec votre version de Scala -->
        </dependency>

        <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-sql_2.12</artifactId>
          <version>3.3.0</version>
        </dependency>


      </dependencies>
    </profile>
    <profile>
      <id>scala-2.11</id>
      <properties>
        <scala.version>2.11.12</scala.version>
        <scala.compat.version>2.11</scala.compat.version>
      </properties>
      <dependencies>
        <dependency>
          <groupId>org.scala-lang</groupId>
          <artifactId>scala-library</artifactId>
          <version>${scala.version}</version>
        </dependency>
        <dependency>
          <groupId>org.scala-lang.modules</groupId>
          <artifactId>scala-xml_${scala.compat.version}</artifactId>
          <version>1.1.1</version>
        </dependency>
        <dependency>
          <groupId>org.scala-lang.modules</groupId>
          <artifactId>scala-parser-combinators_${scala.compat.version}</artifactId>
          <version>1.1.1</version>
        </dependency>

      </dependencies>
    </profile>
    <profile>
      <id>scala-2.10</id>
      <properties>
        <scala.version>2.10.7</scala.version>
        <scala.compat.version>2.10</scala.compat.version>
      </properties>
      <dependencies>
        <dependency>
          <groupId>org.scala-lang</groupId>
          <artifactId>scala-library</artifactId>
          <version>${scala.version}</version>
        </dependency>

      </dependencies>
    </profile>
  </profiles>
  <build>
    <sourceDirectory>src/main/scala</sourceDirectory>
    <testSourceDirectory>src/test/scala</testSourceDirectory>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>3.3</version>
      </plugin>
      <plugin>
        <groupId>net.alchim31.maven</groupId>
        <artifactId>scala-maven-plugin</artifactId>
        <version>3.2.2</version>
        <executions>
          <execution>
            <goals>
              <goal>compile</goal>
              <goal>testCompile</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <args>
            <!-- work-around for https://issues.scala-lang.org/browse/SI-8358 -->
            <arg>-nobootcp</arg>
          </args>
        </configuration>
      </plugin>
    </plugins>
  </build>
</project>
```

---

### **3Ô∏è‚É£ Recharger le projet Maven**
1. **Cliquez sur l‚Äôonglet "Maven"** dans IntelliJ IDEA.
2. Cliquez sur **"Reload All Maven Projects"** (ic√¥ne de rafra√Æchissement).
3. Attendez que Maven t√©l√©charge toutes les d√©pendances.

---

# **3. Ajout du Code Scala**
Dans le dossier `src/main/scala`, cr√©ez un fichier `StockProcessor.scala` et **collez le code suivant** :

```scala
// Importation des biblioth√®ques n√©cessaires

import org.apache.spark.sql.{SparkSession, DataFrame}  // DataFrame est ici correctement import√©
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Encoders


// D√©finition de la case class pour les stocks
case class Stock(
                  dt: String,
                  openprice: Double,
                  highprice: Double,
                  lowprice: Double,
                  closeprice: Double,
                  volume: Double,
                  adjcloseprice: Double
                )

// Objet contenant les m√©thodes pour parser et charger les donn√©es
object StockProcessor {

  def parseStock(str: String): Option[Stock] = {
    val line = str.split(",")

    try {
      Some(Stock(
        line(0),
        line(1).toDouble,
        line(2).toDouble,
        line(3).toDouble,
        line(4).toDouble,
        line(5).toDouble,
        line(6).toDouble
      ))
    } catch {
      case e: Exception =>
        println(s"Erreur de parsing pour la ligne : $str -> ${e.getMessage}")
        None
    }
  }
  def parseRDD(rdd: RDD[String]): RDD[Stock] = {
    val header = rdd.first() // R√©cup√©ration de l'en-t√™te
    rdd
      .filter(_ != header) // Suppression de l'en-t√™te
      .flatMap(parseStock) // Utilisation de flatMap pour ignorer les erreurs
      .cache()
  }
  def main(args: Array[String]): Unit = {
    // Cr√©ation de la session Spark
    val spark = SparkSession.builder()
      .appName("Stock Analysis")
      .master("local[*]") // Mode local
      .getOrCreate()

    import spark.implicits._ // Import pour convertir RDD en DataFrame

    // Charger le fichier CSV et transformer en DataFrame
    val stocksAAPLDF: DataFrame = parseRDD(spark.sparkContext.textFile("C:/Users/rehou/Downloads/AAPL.csv"))
      .toDF() // Conversion en DataFrame
      .cache()

    // Affichage des premi√®res lignes
    stocksAAPLDF.show()
  }
}
```

---

# **‚öô 4. Configuration de l‚ÄôEx√©cution**
### **üîπ Modifier les Configurations d'Ex√©cution**
1. **Cliquez sur le bouton vert ‚ñ∂** en haut.
2. S√©lectionnez **"Edit Configurations..."**.
3. Cliquez sur **"Modify options"**.
4. Cochez **"Allow multiple instances"**.
5. Allez dans *Build and run.* ¬≠> *Select Alternative JRE*
6. **S√©lectionnez Java 8** dans les param√®tres d‚Äôex√©cution.
7. Cliquez sur **Run**

---

# **‚ñ∂ 5. Ex√©cuter le Programme**
1. Cliquez sur **le bouton vert ‚ñ∂** √† c√¥t√© de `main()`.
2. Attendez que Spark d√©marre et affiche les r√©sultats.

---

# **6. R√©sultat Attendu**
```
+----------+---------+---------+---------+---------+-------+-------------+
|       dt |openprice|highprice|lowprice |closeprice|volume|adjcloseprice|
+----------+---------+---------+---------+---------+-------+-------------+
|2023-01-02|  125.02 |  130.05 |  124.52 |  129.43 |200000 |  129.43     |
|2023-01-03|  129.55 |  132.00 |  127.75 |  130.98 |220000 |  130.98     |
|2023-01-04|  131.00 |  135.12 |  130.45 |  134.52 |250000 |  134.52     |
+----------+---------+---------+---------+---------+-------+-------------+
```

---

# **7. Exercice**
1. **Changer le chemin du fichier CSV** en fonction de votre syst√®me.
2. **Ajouter une colonne `prix_moyen`** (`(openprice + closeprice) / 2`).
3. **Appliquer un filtre** pour afficher uniquement les actions avec un `volume > 210000`.



# 8. Annexe 1 - Remarques Importantes 

1Ô∏è‚É£ **Version de Maven** :  
   - **Il est imp√©ratif d‚Äôutiliser Maven 3.9.0**.  
   - **Si vous utilisez une autre version, vous risquez d‚Äôavoir des erreurs de compilation**.  
   - Vous pouvez v√©rifier votre version avec la commande suivante dans le terminal :  
     ```sh
     mvn -version
     ```
   - Si ce n‚Äôest pas la bonne version, mettez √† jour Maven ou t√©l√©chargez **Maven 3.9.0** depuis [Apache Maven](https://maven.apache.org/download.cgi).

---

2Ô∏è‚É£ **Version de Java** :  
   - **Seule la version Java 8 (JDK 1.8) est compatible avec Spark 3.3.0 et Scala 2.12.7.**  
   - **N‚Äôutilisez pas Java 11, 17 ou plus, cela entra√Ænera des erreurs de compatibilit√©**.  
   - V√©rifiez votre version de Java avec la commande :  
     ```sh
     java -version
     ```
   - Si ce n‚Äôest pas Java 8, vous devez l‚Äôinstaller et le d√©finir comme version active.

---

3Ô∏è‚É£ **Configuration d'IntelliJ IDEA** :  
   - **Dans les param√®tres d'ex√©cution du projet, il est obligatoire d‚Äôactiver "Allow multiple instances"**.  
   - Pour cela :  
     1. **Cliquez sur "Run/Debug Configurations"**.  
     2. **S√©lectionnez votre application Spark**.  
     3. **Cochez l‚Äôoption "Allow multiple instances"**.  



# Annexe 1 - arborescence de notre pom.xml

*Cette arborescence dans notre annexe permet de visualiser clairement la hi√©rarchie de votre fichier `pom.xml`, y compris les d√©pendances, les propri√©t√©s, les profils Maven et les plugins utilis√©s.*

```
project
‚îú‚îÄ‚îÄ modelVersion: 4.0.0
‚îú‚îÄ‚îÄ groupId: sample
‚îú‚îÄ‚îÄ artifactId: scala-module-dependency-sample
‚îú‚îÄ‚îÄ version: 1.0-SNAPSHOT
‚îú‚îÄ‚îÄ properties
‚îÇ   ‚îú‚îÄ‚îÄ encoding: UTF-8
‚îú‚îÄ‚îÄ profiles
‚îÇ   ‚îú‚îÄ‚îÄ profile (id: scala-2.12)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ activation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ activeByDefault: true
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ properties
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scala.version: 2.12.7
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scala.compat.version: 2.12
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.scala-lang:scala-library:${scala.version})
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.scala-lang.modules:scala-xml_${scala.compat.version}:1.1.1)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.scala-lang.modules:scala-parser-combinators_${scala.compat.version}:1.1.1)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.scala-lang.modules:scala-swing_${scala.compat.version}:2.0.3)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.apache.spark:spark-core_2.12:3.3.0)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.apache.spark:spark-sql_2.12:3.3.0)
‚îÇ   ‚îú‚îÄ‚îÄ profile (id: scala-2.11)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ properties
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scala.version: 2.11.12
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scala.compat.version: 2.11
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.scala-lang:scala-library:${scala.version})
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.scala-lang.modules:scala-xml_${scala.compat.version}:1.1.1)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.scala-lang.modules:scala-parser-combinators_${scala.compat.version}:1.1.1)
‚îÇ   ‚îú‚îÄ‚îÄ profile (id: scala-2.10)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ properties
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scala.version: 2.10.7
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scala.compat.version: 2.10
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency (org.scala-lang:scala-library:${scala.version})
‚îú‚îÄ‚îÄ build
‚îÇ   ‚îú‚îÄ‚îÄ sourceDirectory: src/main/scala
‚îÇ   ‚îú‚îÄ‚îÄ testSourceDirectory: src/test/scala
‚îÇ   ‚îú‚îÄ‚îÄ plugins
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plugin (org.apache.maven.plugins:maven-compiler-plugin:3.3)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plugin (net.alchim31.maven:scala-maven-plugin:3.2.2)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executions
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ execution
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ goals
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ goal: compile
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ goal: testCompile
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configuration
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ args
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ arg: -nobootcp
```

---
# Annexe 2 - Explication d√©taill√©e de chaque section de notre fichier `pom.xml`
---

*Avec cet annexe, **vous comprendrez comment fonctionne un `pom.xml`** pour un projet **Scala avec Spark et Maven dans IntelliJ IDEA** !*



### **üìå Explication des Sections du `pom.xml`**
```
+----------------------+----------------------------------------------------------+
|      Section        |                          Explication                       |
+----------------------+----------------------------------------------------------+
| project             | D√©crit le projet Maven et contient toutes les autres      |
|                      | sections n√©cessaires pour le build et la gestion des     |
|                      | d√©pendances.                                             |
+----------------------+----------------------------------------------------------+
| modelVersion        | Indique la version du mod√®le de projet Maven utilis√©.     |
|                      | Ici, c'est la version `4.0.0` qui est standard.          |
+----------------------+----------------------------------------------------------+
| groupId             | Identifie le groupe du projet. C'est une sorte de         |
|                      | "namespace". Ici, `sample` est d√©fini.                   |
+----------------------+----------------------------------------------------------+
| artifactId          | Nom unique du projet dans le groupe. Ici,                 |
|                      | `scala-module-dependency-sample` est d√©fini.             |
+----------------------+----------------------------------------------------------+
| version             | Version actuelle du projet. Ici, `1.0-SNAPSHOT` indique   |
|                      | qu‚Äôil s‚Äôagit d‚Äôune version en d√©veloppement.             |
+----------------------+----------------------------------------------------------+
| properties          | Contient des variables globales pour le projet Maven.     |
|                      | Exemple : `encoding=UTF-8` pour g√©rer l‚Äôencodage.        |
+----------------------+----------------------------------------------------------+
| profiles            | Permet de d√©finir plusieurs configurations pour un m√™me   |
|                      | projet. Chaque `profile` peut contenir des d√©pendances   |
|                      | diff√©rentes selon les besoins.                           |
+----------------------+----------------------------------------------------------+
| profile scala-2.12  | Configuration sp√©cifique pour Scala 2.12.7.               |
|                      | Il d√©finit les d√©pendances pour cette version.           |
+----------------------+----------------------------------------------------------+
| profile scala-2.11  | Configuration sp√©cifique pour Scala 2.11.12.              |
|                      | Il d√©finit ses propres d√©pendances.                      |
+----------------------+----------------------------------------------------------+
| profile scala-2.10  | Configuration sp√©cifique pour Scala 2.10.7.               |
|                      | Il d√©finit ses propres d√©pendances.                      |
+----------------------+----------------------------------------------------------+
| dependencies        | Liste des biblioth√®ques n√©cessaires pour ex√©cuter le code.|
|                      | Exemple : Spark, Scala standard, XML, Parser, etc.      |
+----------------------+----------------------------------------------------------+
| build              | Contient les instructions pour compiler et ex√©cuter le    |
|                      | projet. Il inclut des plugins qui facilitent le travail.|
+----------------------+----------------------------------------------------------+
| sourceDirectory     | D√©finit o√π se trouvent les fichiers Scala.                |
|                      | Ici, `src/main/scala` est d√©fini comme source.          |
+----------------------+----------------------------------------------------------+
| testSourceDirectory | D√©finit o√π se trouvent les fichiers de tests Scala.      |
|                      | Ici, `src/test/scala` est d√©fini.                        |
+----------------------+----------------------------------------------------------+
| plugins             | Liste des outils Maven qui aident au processus de build.  |
+----------------------+----------------------------------------------------------+
| maven-compiler-plugin | Plugin pour compiler le code Java/Scala dans le projet. |
|                      | Ici, la version utilis√©e est `3.3`.                      |
+----------------------+----------------------------------------------------------+
| scala-maven-plugin  | Plugin qui permet de compiler et ex√©cuter du Scala       |
|                      | dans un projet Maven. Version `3.2.2` utilis√©e ici.      |
+----------------------+----------------------------------------------------------+
| executions         | D√©finit quelles t√¢ches Maven doit ex√©cuter automatiquement.|
|                      | Exemple : compilation et test du code Scala.            |
+----------------------+----------------------------------------------------------+
| goals               | Liste des actions √† ex√©cuter lors du build.               |
|                      | Ici : `compile` (compiler le code), `testCompile` (tests).|
+----------------------+----------------------------------------------------------+
| configuration       | Contient des param√®tres avanc√©s pour les plugins.         |
|                      | Exemple : `-nobootcp` pour √©viter certains conflits.    |
+----------------------+----------------------------------------------------------+
```

---

### **üõ† D√©tails des Versions Scala utilis√©es**
```
+------------+------------------------------+--------------------------+
| Version    | Compatibilit√© avec Spark     | Commentaire               |
+------------+------------------------------+--------------------------+
| Scala 2.12 | Compatible avec Spark 3.3.0  | Utilis√©e par d√©faut       |
+------------+------------------------------+--------------------------+
| Scala 2.11 | Ancienne version support√©e   | N√©cessaire pour certains  |
|            | mais obsol√®te pour Spark 3.x | projets legacy            |
+------------+------------------------------+--------------------------+
| Scala 2.10 | Tr√®s ancienne version        | Rarement utilis√©e         |
+------------+------------------------------+--------------------------+
```

---

### **üöÄ Explication des Profils (`profiles`)**
Les **profils Maven** permettent d‚Äôavoir **diff√©rentes configurations** pour un m√™me projet. Ici, trois profils sont d√©finis :
1. **Scala 2.12 (Par d√©faut)** :  
   - Active les d√©pendances pour Scala 2.12.7.  
   - Utilise Spark 3.3.0.  

2. **Scala 2.11** :  
   - Charge les d√©pendances pour Scala 2.11.12.  
   - Peut √™tre utile pour des projets plus anciens.  

3. **Scala 2.10** :  
   - D√©finit les biblioth√®ques compatibles avec Scala 2.10.7.  
   - Tr√®s peu utilis√© aujourd‚Äôhui.  

---

### **üì¶ Explication des D√©pendances (`dependencies`)**
```
+------------------------------------------+--------------------------------------------+
| D√©pendance                               | Explication                                |
+------------------------------------------+--------------------------------------------+
| org.scala-lang:scala-library             | Biblioth√®que standard Scala                |
|                                          | Permet d'ex√©cuter du code Scala            |
+------------------------------------------+--------------------------------------------+
| org.scala-lang.modules:scala-xml         | Gestion des fichiers XML en Scala         |
+------------------------------------------+--------------------------------------------+
| org.scala-lang.modules:scala-parser-...  | Librairie pour parser du texte            |
+------------------------------------------+--------------------------------------------+
| org.apache.spark:spark-core_2.12         | Noyau de Spark pour Scala 2.12            |
+------------------------------------------+--------------------------------------------+
| org.apache.spark:spark-sql_2.12          | Biblioth√®que SQL de Spark                 |
+------------------------------------------+--------------------------------------------+
```

---

### **üõ† Explication des Plugins (`plugins`)**
1. **maven-compiler-plugin**  
   - Utilis√© pour compiler le code Java et Scala.  
   - Version `3.3` utilis√©e ici.  

2. **scala-maven-plugin**  
   - Permet de compiler du **Scala** avec **Maven**.  
   - Version `3.2.2`.  
   - D√©finit que **`compile` et `testCompile`** doivent √™tre ex√©cut√©s.  

---

### **üéØ Ex√©cutions (`executions`)**
L‚Äôex√©cution d√©finit **quelles t√¢ches** Maven doit r√©aliser lors du build :
- **compile** : Compile le code Scala.  
- **testCompile** : Compile les tests Scala.  

---

### **‚öô Configuration**
Dans la section **configuration**, l‚Äôargument `-nobootcp` est utilis√© pour √©viter certains conflits de classpath avec Scala.

---

### **üìå R√©capitulatif**
- **Le projet utilise trois versions de Scala (2.12, 2.11, 2.10)** mais **Scala 2.12 est activ√© par d√©faut**.  
- **Les d√©pendances Spark et Scala sont charg√©es dynamiquement selon le profil s√©lectionn√©**.  
- **Les plugins permettent de compiler et ex√©cuter du code Scala avec Maven**.  
- **L‚Äôex√©cution de Maven compile le code et les tests Scala**.  


# Annexe 3 - Expliaction d√©taill√©e du code



```scala
// Importation des biblioth√®ques n√©cessaires pour utiliser Spark et manipuler les donn√©es
import org.apache.spark.sql.{SparkSession, DataFrame}  // SparkSession est utilis√© pour cr√©er une session Spark, DataFrame pour manipuler les donn√©es
import org.apache.spark.rdd.RDD  // RDD (Resilient Distributed Dataset) est une abstraction de donn√©es distribu√©es dans Spark
import org.apache.spark.sql.Encoders  // Utilis√© pour convertir des objets Scala en format compatible avec les DataFrames

// D√©finition d'une case class `Stock` qui repr√©sente un enregistrement d'action boursi√®re
case class Stock(
                  dt: String,           // Date de l'enregistrement
                  openprice: Double,    // Prix d'ouverture
                  highprice: Double,    // Prix le plus haut
                  lowprice: Double,     // Prix le plus bas
                  closeprice: Double,   // Prix de cl√¥ture
                  volume: Double,       // Volume √©chang√©
                  adjcloseprice: Double // Prix ajust√© de cl√¥ture
                )

// D√©finition d'un objet `StockProcessor` contenant les m√©thodes pour analyser et charger les donn√©es
object StockProcessor {

  // Fonction qui prend une ligne CSV (sous forme de String) et la convertit en objet `Stock`
  def parseStock(str: String): Option[Stock] = {
    val line = str.split(",")  // S√©pare la ligne en utilisant la virgule comme s√©parateur

    try {
      Some(Stock(   // Essaie de cr√©er un objet Stock avec les valeurs extraites
        line(0),            // Date (dt)
        line(1).toDouble,   // Prix d'ouverture (openprice)
        line(2).toDouble,   // Prix le plus haut (highprice)
        line(3).toDouble,   // Prix le plus bas (lowprice)
        line(4).toDouble,   // Prix de cl√¥ture (closeprice)
        line(5).toDouble,   // Volume √©chang√© (volume)
        line(6).toDouble    // Prix ajust√© de cl√¥ture (adjcloseprice)
      ))
    } catch {
      case e: Exception =>  // En cas d'erreur lors de la conversion
        println(s"Erreur de parsing pour la ligne : $str -> ${e.getMessage}") // Affiche un message d'erreur
        None  // Retourne `None` pour ignorer cette ligne incorrecte
    }
  }

  // Fonction qui prend un RDD[String] et le convertit en un RDD[Stock]
  def parseRDD(rdd: RDD[String]): RDD[Stock] = {
    val header = rdd.first() // R√©cup√©ration de la premi√®re ligne du fichier (l'en-t√™te)
    rdd
      .filter(_ != header) // Supprime la premi√®re ligne (l'en-t√™te) pour ne pas la traiter
      .flatMap(parseStock) // Applique la fonction `parseStock` √† chaque ligne et ignore les erreurs
      .cache() // Met en cache le RDD pour am√©liorer les performances
  }

  def main(args: Array[String]): Unit = {
    // Cr√©ation de la session Spark (point d'entr√©e pour utiliser Spark)
    val spark = SparkSession.builder()
      .appName("Stock Analysis") // Nom de l'application Spark
      .master("local[*]") // Mode local (utilise tous les c≈ìurs disponibles de la machine locale)
      .getOrCreate() // Cr√©e ou r√©cup√®re une session Spark existante

    import spark.implicits._ // Importation implicite pour convertir un RDD en DataFrame

    // Charger le fichier CSV et le transformer en DataFrame
    val stocksAAPLDF: DataFrame = parseRDD(spark.sparkContext.textFile("C:/Users/rehou/Downloads/AAPL.csv"))
      .toDF() // Conversion du RDD[Stock] en DataFrame
      .cache() // Met en cache les donn√©es pour √©viter de relire le fichier √† chaque requ√™te

    // Affichage des premi√®res lignes du DataFrame sous forme de tableau
    stocksAAPLDF.show()
  }
}
```

---

### **Explication G√©n√©rale du Code**
1. **Importation des biblioth√®ques** :  
   - SparkSession est utilis√© pour cr√©er une session Spark.
   - DataFrame est une abstraction de Spark SQL pour manipuler les donn√©es tabulaires.
   - RDD (Resilient Distributed Dataset) est utilis√© pour la gestion des donn√©es en m√©moire et leur traitement distribu√©.

2. **D√©finition de la case class `Stock`** :  
   - Permet de structurer les donn√©es en objets Scala.
   - Facilite leur manipulation et leur conversion en DataFrame.

3. **Fonction `parseStock(str: String)`** :  
   - Convertit une ligne de texte CSV en objet `Stock`.
   - G√®re les erreurs en cas de format incorrect.

4. **Fonction `parseRDD(rdd: RDD[String])`** :  
   - Lit un fichier CSV sous forme de `RDD[String]`.
   - Supprime l'en-t√™te du fichier.
   - Convertit chaque ligne en objet `Stock`.
   - Met en cache les donn√©es pour am√©liorer les performances.

5. **Fonction `main(args: Array[String])`** :  
   - Cr√©e une session Spark.
   - Charge les donn√©es d'un fichier CSV.
   - Convertit le fichier en DataFrame.
   - Affiche les premi√®res lignes du DataFrame.

---

### ** Points Cl√©s √† Retenir**
- **RDD vs DataFrame** :  
  - Un **RDD** est une collection distribu√©e d'objets Scala.
  - Un **DataFrame** est une table optimis√©e pour Spark SQL (plus performant que les RDDs).

- **`cache()`** :  
  - √âvite la relecture des donn√©es depuis le fichier en stockant les r√©sultats en m√©moire.

- **Utilisation de `Option[Stock]`** :  
  - Permet de g√©rer les erreurs de parsing en √©vitant d'inclure des lignes corrompues dans les r√©sultats.

# Annexe 4 -  parseStock vs parseRDD

Nous ne sommes pas oblig√©s de transformer en RDD avant de cr√©er un DataFrame dans Spark. On le fait ici pour contr√¥ler manuellement le parsing des donn√©es avant de les convertir en DataFrame.


Non, nous ne sommes **pas oblig√©s** de transformer en **RDD** avant de cr√©er un **DataFrame** dans Spark. On le fait ici pour **contr√¥ler manuellement le parsing des donn√©es** avant de les convertir en **DataFrame**.



## ** Explication du Workflow**
L‚Äôobjectif est de **charger un fichier CSV** en Spark et de le convertir en un format utilisable pour l‚Äôanalyse des donn√©es.

### ** √âtapes du Workflow**
1. **Lecture du fichier CSV brut** :  
   - Spark charge les donn√©es sous forme de **RDD[String]** (chaque ligne est une cha√Æne de caract√®res).
   
2. **Traitement des erreurs via `parseStock(str: String)`** :  
   - Convertit une ligne CSV en un objet `Stock`.  
   - Ignore les lignes mal format√©es gr√¢ce √† `Option[Stock]`.  

3. **Transformation compl√®te avec `parseRDD(rdd: RDD[String])`** :  
   - Applique `parseStock` sur tout le fichier.
   - Supprime l‚Äôen-t√™te du fichier.
   - Filtre les erreurs.
   - Convertit ensuite en DataFrame.

---

## ** Pourquoi `parseStock(str: String)` ?**
- **Probl√®me** : Quand on charge un CSV, les donn√©es sont brutes (du texte).  
- **Solution** : `parseStock` transforme une ligne CSV (`String`) en **objet structur√© `Stock`**.
- **Gestion d‚Äôerreurs** :  
  - Si une ligne a **des valeurs invalides** (`"abc"` au lieu de `12.34`), elle est ignor√©e **au lieu de faire planter Spark**.
  - Cela √©vite d'avoir **des erreurs de parsing massives** si le fichier contient des donn√©es incorrectes.

### ** Exemple d‚ÄôUtilisation**
```scala
val ligne = "2024-02-25,100.5,105.0,98.3,104.2,200000,104.2"
val stock = parseStock(ligne)
println(stock)  
// R√©sultat : Some(Stock(2024-02-25,100.5,105.0,98.3,104.2,200000,104.2))
```
Mais si on passe une ligne invalide :
```scala
val ligneErronee = "2024-02-25,100.5,abc,98.3,104.2,200000,104.2"
val stockErrone = parseStock(ligneErronee)
println(stockErrone)  
// R√©sultat : None (et affiche un message d'erreur)
```
Cela **√©vite que Spark plante** √† cause d'une seule erreur.

---

## ** Pourquoi `parseRDD(rdd: RDD[String])` ?**
- **Probl√®me** : Spark charge un CSV comme un **RDD de Strings**.  
- **Solution** : `parseRDD` applique `parseStock` **√† toutes les lignes du fichier**.
- **Gestion de l‚Äôen-t√™te** :  
  - Supprime la **premi√®re ligne** qui contient `"Date,Open,High,Low,Close,Volume,AdjClose"` (inutile dans le traitement).
- **Am√©liore les performances** :  
  - **RDD cach√© (`.cache()`)** :  
    - Stocke les donn√©es en m√©moire pour √©viter **de relire le fichier plusieurs fois**.

### ** Exemple d‚ÄôUtilisation**
```scala
val rddBrut = spark.sparkContext.parallelize(Seq(
  "Date,Open,High,Low,Close,Volume,AdjClose",  // En-t√™te √† supprimer
  "2024-02-25,100.5,105.0,98.3,104.2,200000,104.2",
  "2024-02-26,102.0,106.5,100.0,105.5,180000,105.5",
  "2024-02-27,abc,107.0,101.0,106.0,210000,106.0"  // Erreur ici !
))

val rddStock = parseRDD(rddBrut)
rddStock.collect().foreach(println)
```
**Sortie :**
```
Stock(2024-02-25,100.5,105.0,98.3,104.2,200000,104.2)
Stock(2024-02-26,102.0,106.5,100.0,105.5,180000,105.5)
Erreur de parsing pour la ligne : 2024-02-27,abc,107.0,101.0,106.0,210000,106.0 -> For input string: "abc"
```
üëâ La **ligne invalide est ignor√©e** sans faire crasher Spark !

---

## ** Conclusion : Pourquoi ce Workflow ?**
| √âtape                 | Pourquoi ?                                                   |
|----------------------|-----------------------------------------------------------|
| **1. Lire un fichier CSV**  | Spark traite les lignes comme du texte brut (`RDD[String]`). |
| **2. `parseStock`**         | Transforme **chaque ligne** en un objet `Stock` structur√©.  |
| **3. `parseRDD`**           | - Supprime l‚Äôen-t√™te <br>- G√®re les erreurs <br>- Transforme en `RDD[Stock]` |
| **4. Convertir en DataFrame** | Permet d‚Äôutiliser Spark SQL (`toDF()`).                           |

üëâ **On transforme en RDD pour g√©rer les erreurs et structurer les donn√©es avant de les convertir en DataFrame !** üöÄ



<a name="annexe-6--workflow-du-programme-scala-avec-spark"></a>


---
# Annexe 6 -  Workflow du Programme Scala avec Spark
---


```
+---------------------------+
|   D√©finition de la classe |
|        Stock.scala        |
+---------------------------+
         ‚¨á (Structure de donn√©es)
+------------------------------------------------------+
| case class Stock(dt, openprice, highprice, lowprice,|
|                 closeprice, volume, adjcloseprice)  |
| - Repr√©sente une ligne du fichier CSV              |
| - Facilite la conversion en DataFrame               |
+------------------------------------------------------+
         ‚¨á (Transformation du fichier CSV en RDD)
+-----------------------------------------------+
|  Lecture du fichier CSV brut en RDD[String]  |
|  ‚Üí Chaque ligne est une String               |
|                                               |
|  Exemple:                                     |
|  "Date,Open,High,Low,Close,Volume,AdjClose"  |  <- En-t√™te (√† ignorer)
|  "2024-02-25,100.5,105.0,98.3,104.2,200000,104.2" |
|  "2024-02-26,102.0,106.5,100.0,105.5,180000,105.5" |
+-----------------------------------------------+
         ‚¨á (Nettoyage et Parsing des donn√©es)
+-----------------------------------------------+
|  Fonction parseStock(str: String)            |
|  ‚Üí Transforme une ligne CSV en objet Stock   |
|  ‚Üí G√®re les erreurs (ignore les lignes invalides) |
|                                               |
|  Exemple :                                    |
|  Entr√©e : "2024-02-25,100.5,105.0,98.3,104.2,200000,104.2"  |
|  Sortie : Stock(2024-02-25,100.5,105.0,98.3,104.2,200000,104.2) |
+-----------------------------------------------+
         ‚¨á (Application du parsing √† tout le fichier)
+-----------------------------------------------+
|  Fonction parseRDD(rdd: RDD[String])         |
|  ‚Üí Supprime l'en-t√™te du fichier CSV         |
|  ‚Üí Applique parseStock √† toutes les lignes   |
|  ‚Üí Ignore les lignes incorrectes             |
|  ‚Üí Renvoie un RDD[Stock]                     |
|                                               |
|  Exemple :                                    |
|  Entr√©e : RDD[String] (chaque ligne du CSV)  |
|  Sortie : RDD[Stock] (objets Stock bien structur√©s) |
+-----------------------------------------------+
         ‚¨á (Conversion en DataFrame)
+-----------------------------------------------+
|  Transformation du RDD[Stock] en DataFrame   |
|  ‚Üí Permet d'utiliser Spark SQL               |
|  ‚Üí Optimis√© pour les requ√™tes rapides        |
|                                               |
|  Exemple :                                    |
|  Entr√©e : RDD[Stock]                          |
|  Sortie : DataFrame                           |
|                                               |
|  +----------+---------+---------+--------+   |
|  |    dt    |openprice|highprice|lowprice|   |
|  +----------+---------+---------+--------+   |
|  |2024-02-25|  100.5  |  105.0  |  98.3  |   |
|  |2024-02-26|  102.0  |  106.5  | 100.0  |   |
|  +----------+---------+---------+--------+   |
+-----------------------------------------------+
         ‚¨á (Affichage des r√©sultats)
+-----------------------------------------------+
|  stocksAAPLDF.show()                         |
|  ‚Üí Affiche les premi√®res lignes du DataFrame |
+-----------------------------------------------+
```

---

### **üõ† R√©sum√© en √âtapes**
| **√âtape** | **Explication** |
|-----------|----------------|
| **1. D√©finition de `Stock`** | Cr√©e une classe Scala qui structure les donn√©es |
| **2. Lecture du CSV** | Charge un fichier sous forme de texte brut (`RDD[String]`) |
| **3. `parseStock(str: String)`** | Transforme une ligne en objet `Stock` et g√®re les erreurs |
| **4. `parseRDD(rdd: RDD[String])`** | Supprime l‚Äôen-t√™te et applique `parseStock` √† tout le fichier |
| **5. Conversion en DataFrame** | Convertit `RDD[Stock]` en `DataFrame` optimis√© |
| **6. Affichage des donn√©es** | Affiche les 20 premi√®res lignes avec `show()` |

---

### **Pourquoi ce Workflow ?**
- **RDD permet de contr√¥ler les erreurs** avant la conversion en DataFrame.
- **Le parsing manuel** (`parseStock`) √©vite que Spark plante si une ligne est invalide.
- **Les DataFrames sont plus rapides et optimis√©s pour Spark SQL**.

