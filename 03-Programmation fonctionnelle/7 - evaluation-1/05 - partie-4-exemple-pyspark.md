Voici une **table complète** contenant chaque portion de code (ou « bout de code ») numérotée, accompagnée d’une **explication générale** de ce qu’elle fait. L’objectif est d’offrir une vue d’ensemble claire et structurée pour chacun des 40 segments identifiés.

| **N°** | **Code** | **Explication** |
|-------:|:--------:|:---------------|
| **1** | <pre># DBTITLE 1,Cmd1<br># MAGIC %run ./Authorization.py</pre> | Exécute le script `Authorization.py` pour configurer ou autoriser l’accès aux ressources (généralement Azure Data Lake). Cela initialise l’environnement en chargeant les informations de connexion requises. |
| **2** | <pre># File location and type<br>file_location = "abfss://containeurmovieal3@comptedestockagemovieal3.dfs.core.windows.net/links.csv"<br>file_type = "csv"<br>infer_schema = "true"<br>first_row_is_header = "true"<br>delimiter = ","<br>df_movies = spark.read.format(file_type) \    .option("inferSchema", infer_schema) \    .option("header", first_row_is_header) \    .option("sep", delimiter) \    .load(file_location)<br>display(df_movies)</pre> | 1) Définit la source de données : **chemin du fichier** `links.csv`, **type** CSV, et options (détection de schéma, en-têtes, séparateur).<br>2) Charge le CSV dans un **DataFrame Spark** nommé `df_movies`.<br>3) Affiche le résultat avec `display(df_movies)`. |
| **3** | <pre># DBTITLE 1,Cmd3<br>import datetime<br>import pyspark.sql.functions as f<br>import pyspark.sql.types<br>import pandas as pd</pre> | Importe les **bibliothèques Python** nécessaires :<br>- `datetime` pour la manipulation de dates<br>- `pyspark.sql.functions` pour les fonctions Spark<br>- `pyspark.sql.types` pour les types de données Spark<br>- `pandas` pour la conversion ou la manipulation de données en DataFrame Pandas. |
| **4** | <pre># DBTITLE 1,Cmd4<br>from pyspark.sql.functions import year, month, dayofmonth<br>from pyspark.sql.functions import unix_timestamp, from_unixtime<br>from pyspark.sql import Window<br>from pyspark.sql.functions import rank, min</pre> | Importe des **fonctions Spark** supplémentaires :<br>- `year`, `month`, `dayofmonth` pour extraire des parties de dates<br>- `unix_timestamp`, `from_unixtime` pour manipuler des timestamps<br>- `Window` pour définir des fenêtres de calcul (window functions)<br>- `rank`, `min` pour classer et trouver des valeurs minimales sur des fenêtres. |
| **5** | <pre># DBTITLE 1,Cmd5<br>file_location = "abfss://containeurmovieal3@comptedestockagemovieal3.dfs.core.windows.net/movies.csv"<br>file_type = "csv"<br>infer_schema = "true"<br>first_row_is_header = "true"<br>delimiter = ","<br>df_movies = spark.read.format(file_type) \    .option("inferSchema", infer_schema) \    .option("header", first_row_is_header) \    .option("sep", delimiter) \    .load(file_location)<br>display(df_movies)</pre> | Charge un nouveau fichier CSV (**movies.csv**) depuis Azure Data Lake, en appliquant les mêmes options (schéma, en-têtes, séparateur). Stocke le résultat dans `df_movies`, puis affiche le DataFrame. |
| **6** | <pre># DBTITLE 1,Cmd6<br>temp_table_name = "movies_csv"<br>df_movies.createOrReplaceTempView(temp_table_name)</pre> | Crée ou remplace une **vue temporaire** nommée `movies_csv`, permettant de **faire des requêtes SQL** directement sur le DataFrame `df_movies`. |
| **7** | <pre># DBTITLE 1,Cmd7<br># MAGIC %sql<br># MAGIC select * from `movies_csv`</pre> | Utilise la **magie** `%sql` propre à Databricks pour exécuter une **requête SQL** : sélectionne toutes les colonnes de la vue `movies_csv`. |
| **8** | <pre># DBTITLE 1,Cmd8<br>permanent_table_name = "movies_csv"</pre> | Prépare la **variable** `permanent_table_name` avec le même nom `movies_csv`. Cela peut être utilisé plus tard pour des opérations de persistance dans Databricks ou un catalog. |
| **9** | <pre># DBTITLE 1,Cmd9<br>links="abfss://containeurmovieal3@comptedestockagemovieal3.dfs.core.windows.net/links.csv"<br>df_links = spark.read.format(file_type) \    .option("inferShema", infer_schema) \    .option("header", first_row_is_header) \    .option("sep", delimiter) \    .load(links)<br><br>display(df_links)</pre> | Charge à nouveau le fichier **links.csv**, cette fois dans la variable `df_links`. Il utilise toujours les mêmes options (schéma, en-têtes, séparateur). Affiche le résultat avec `display(df_links)`. |
| **10** | <pre># DBTITLE 1,Cmd10<br>tags = "abfss://containeurmovieal3@comptedestockagemovieal3.dfs.core.windows.net/tags.csv"<br>df_tags = spark.read.format(file_type) \    .option("inferShema", infer_schema) \    .option("header", first_row_is_header) \    .option("sep", delimiter) \    .load(tags)<br><br>display(df_tags)</pre> | Charge le fichier **tags.csv** dans un DataFrame `df_tags`. Même logique : schéma, en-têtes, séparateur. Affiche ensuite le contenu. |
| **11** | <pre># DBTITLE 1,Cmd11<br>ratings = "abfss://containeurmovieal3@comptedestockagemovieal3.dfs.core.windows.net/ratings.csv"<br>df_ratings = spark.read.format(file_type) \    .option("inferShema", infer_schema) \    .option("header", first_row_is_header) \    .option("sep", delimiter) \    .load(ratings)<br><br>display(df_ratings)</pre> | Charge le fichier **ratings.csv** dans `df_ratings`. Pareil : inférence de schéma, header, virgule comme séparateur. Affiche enfin les données chargées. |
| **12** | <pre># DBTITLE 1,Cmd12<br># count of records<br>print(df_movies.count())<br>display(df_movies)</pre> | 1) Affiche sur la console le **nombre de lignes** dans `df_movies` en utilisant `.count()`.<br>2) Montre le contenu de `df_movies` via `display(df_movies)`. |
| **13** | <pre># DBTITLE 1,Cmd13<br>df_movies_with_ratings = df_movies.join(df_ratings,'movieID','left')<br>display(df_movies_with_ratings)</pre> | **Jointure** entre `df_movies` et `df_ratings` sur la colonne `movieID` (type **LEFT JOIN**). Permet d’associer les informations de film avec leurs notations. Affiche le DataFrame résultant. |
| **14** | <pre># DBTITLE 1,Cmd14<br>df_movies_no_dups = df_movies_with_ratings.groupby('movieId').count()<br>display(df_movies_no_dups)</pre> | Regroupe par `movieId` et compte le nombre d’éléments. Sert souvent à **détecter les doublons** ou comprendre la distribution du nombre de lignes par film. |
| **15** | <pre># DBTITLE 1,Cmd15<br>df_movies_with_ratings = df_movies_with_ratings.join(df_tags,['movieId'], 'inner')<br>display(df_movies_with_ratings)</pre> | Jointure **INNER** entre le DataFrame contenant films + notations (`df_movies_with_ratings`) et les tags (`df_tags`) sur `movieId`. On obtient ainsi le DataFrame final intégrant **films, notes et tags**. |
| **16** | <pre># DBTITLE 1,Cmd16<br>df_ratings_tags = df_ratings.join(df_tags,['movieId'], 'inner')<br>display(df_movies_with_ratings)</pre> | 1) Crée `df_ratings_tags` en **associant** `df_ratings` et `df_tags` (INNER JOIN).<br>2) Affiche ensuite `df_movies_with_ratings` (probablement pour vérifier la cohérence de la jointure précédente). |
| **17** | <pre># DBTITLE 1,Cmd17<br>display(df_ratings)</pre> | Affiche le contenu du DataFrame `df_ratings` (c’est-à-dire la table des notations). Permet de **visualiser** les informations avant la suite du traitement. |
| **18** | <pre># DBTITLE 1,Cmd18<br>df_ratings</pre> | Simple appel à la variable `df_ratings`. Généralement, cela ne produit pas d’affichage dans Databricks. C’est un **DataFrame** Spark. |
| **19** | <pre># DBTITLE 1,Cmd19<br>df_ratings = df_ratings.withColumn("tsDate", f.from_unixtime("timestamp"))</pre> | Ajoute une **nouvelle colonne** nommée `tsDate` convertissant la colonne `timestamp` brut en **format date lisible** avec `from_unixtime(...)`. |
| **20** | <pre># DBTITLE 1,Cmd20<br>display(df_ratings)</pre> | Affiche à nouveau `df_ratings` après avoir **ajouté la colonne** `tsDate` pour valider la transformation. |
| **21** | <pre># DBTITLE 1,Cmd21<br>df_ratings = df_ratings.select('userId', 'movieId', 'rating',<br>   f.to_date(unix_timestamp('tsDate', 'yyyy-MM-dd HH:MM:SS').cast('timestamp')).alias('rating_date'))</pre> | Sélectionne uniquement les colonnes utiles : `userId`, `movieId`, `rating` et crée une colonne `rating_date` en convertissant `tsDate` (UNIX Timestamp) au **format date** `yyyy-MM-dd`. |
| **22** | <pre># DBTITLE 1,Cmd22<br>spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")<br>display(df_ratings)</pre> | 1) Modifie la **configuration Spark** pour gérer certains cas de parsing date/heure (compatibilité d’anciennes versions).<br>2) Affiche ensuite `df_ratings` pour vérifier l’effet de cette configuration. |
| **23** | <pre># DBTITLE 1,Cmd23<br>from pyspark.sql.functions import mean as mean_;<br>df_avg_ratings = df_ratings.groupBy('movieId').agg(mean_('rating'))<br>display(df_avg_ratings)</pre> | 1) Importe la fonction `mean` en l’aliasant `mean_`.<br>2) Calcule la **moyenne des notes** (`rating`) par `movieId`.<br>3) Affiche le résultat avec `display(...)`. |
| **24** | <pre># DBTITLE 1,Cmd24<br>df = df_avg_ratings.join(df_movies,'movieId','inner')<br>df = df.withColumnRenamed('avg(rating)','avg_rating')<br>display(df)</pre> | 1) Joint les **moyennes de notes** (`df_avg_ratings`) avec les infos de films (`df_movies`) via `movieId`.<br>2) Renomme la colonne `avg(rating)` en `avg_rating` pour plus de clarté.<br>3) Affiche le DataFrame final (`df`). |
| **25** | <pre># DBTITLE 1,Cmd25<br>df_total_rating = df_ratings.groupBy('movieID').count()<br>display(df_total_rating)</pre> | **Compte le nombre d’évaluations** par `movieID`. Affiche ensuite cette table pour connaître le volume de ratings par film. |
| **26** | <pre># DBTITLE 1,Cmd26<br>df_total_rating = df_total_rating.filter(df_total_rating['count'] > 5)<br>df_ratings_filtered = df_ratings.join(df_total_rating, 'movieID', 'inner')</pre> | 1) **Filtre** pour conserver uniquement les films avec plus de **5 évaluations**.<br>2) Joint ensuite `df_ratings` avec ce filtrage pour obtenir `df_ratings_filtered`, c’est-à-dire seulement les notes de films ayant plus de 5 évaluations. |
| **27** | <pre># DBTITLE 1,Cmd27<br>display(df_total_rating)<br>print(df_total_rating.count())</pre> | 1) Affiche la liste des `movieID` répondant au critère (> 5 évaluations).<br>2) Affiche le **nombre** de lignes restantes (films filtrés). |
| **28** | <pre># DBTITLE 1,Cmd28<br>from pyspark.sql.functions import max as max_, col<br><br>df_rating_per_user = df_ratings_filtered.select('userID','movieID','rating').groupBy('userID','movieID').agg(max_('rating').cast("float").alias("max_rating"))<br>df_rating_per_user_movie = df_rating_per_user.join(df_movies, 'movieID', 'inner')</pre> | 1) Importe la fonction `max` sous l’alias `max_`.<br>2) Sélectionne `userID`, `movieID`, `rating` et **regroupe** par `userID` et `movieID`, en prenant la **note maximale** (`max_rating`).<br>3) Joint ensuite ce résultat avec `df_movies` pour rattacher les informations du film. |
| **29** | <pre># DBTITLE 1,Cmd29<br>df_rating_per_user_movie = df_rating_per_user_movie.withColumnRenamed('max(rating)','max_rating')<br>display(df_rating_per_user_movie)</pre> | 1) Renomme (par cohérence) la colonne `'max(rating)'` en `'max_rating'` (au cas où).<br>2) Affiche le résultat final, où chaque utilisateur a sa note maximale pour chaque film. |
| **30** | <pre># DBTITLE 1,Cmd30<br>df_rating_per_user_movie</pre> | Affiche simplement le **DataFrame** `df_rating_per_user_movie`. Dans Databricks, cela ne produit pas forcément un tableau ; c’est juste la variable. |
| **31** | <pre>df_rating = df_rating_per_user_movie.groupBy('userId','movieId','title','genres').max('max_rating')</pre> | Calcule, pour chaque `(userId, movieId, title, genres)`, la **valeur maximale** de `max_rating`. C’est une nouvelle agrégation permettant de faire des regroupements plus poussés. |
| **32** | <pre>display(df_rating)</pre> | Affiche le **DataFrame** `df_rating`. |
| **33** | <pre># Users with movies with > 4 ratings<br>df_rating = df_rating.withColumnRenamed('max(max_rating)','max_rating')<br>df_rating = df_rating.filter(df_rating['max_rating'] >= 4)<br>display(df_rating)</pre> | 1) Renomme la colonne `'max(max_rating)'` en `'max_rating'`.<br>2) Filtre pour ne garder que les **films notés au moins 4**.<br>3) Affiche le résultat pour voir les films et utilisateurs satisfaits (≥4). |
| **34** | <pre># Identify best movies per genre<br>df_rating_per_genre = df_rating.groupBy('genres','title').count()<br>display(df_rating_per_genre)</pre> | Regroupe par `(genres, title)` et compte le nombre de lignes (probablement le nombre d’utilisateurs qui ont noté ≥4). Permet d’identifier quels films sont considérés « bons » dans chaque genre. |
| **35** | <pre># Identify genre of user<br>df_rating_genre = df_rating.select('userId','title','genres').groupBy('userId','genres').count()</pre> | Pour chaque utilisateur, détermine combien de films (et lesquels) ils ont dans tel ou tel **genre**. Cela peut servir à inférer les préférences de genre par utilisateur. |
| **36** | <pre>display(df_rating_genre)</pre> | Affiche `df_rating_genre` pour visualiser combien de films par genre un utilisateur aime ou a noté. |
| **37** | <pre>df_recent_movie = df_ratings.groupBy('userId','movieId').agg(f.max(df_ratings['rating_date']))</pre> | Détermine, pour chaque `(userId, movieId)`, la **date la plus récente** de notation (`rating_date`). Indique la dernière fois qu’un utilisateur a noté un film particulier. |
| **38** | <pre>display(df_recent_movie)</pre> | Affiche le **DataFrame** `df_recent_movie`, permettant de voir la date maximale par `(userId, movieId)`. |
| **39** | <pre># Latest Trending movies (Overall)<br>df_rating</pre> | Le commentaire suggère qu’on veut identifier les **tendances récentes**. Le code montre la variable `df_rating`, qui contient les films à note >= 4. Aucune opération ne suit, peut-être un point de départ pour une future analyse. |
| **40** | <pre>df_ratings_per_genre = df.groupBy('genres').avg('avg_rating')<br>display(df_ratings_per_genre)</pre> | 1) Regroupe le DataFrame `df` (qui contient `avg_rating` par film) par `genres`, puis calcule la **moyenne de la note moyenne** (`avg_rating`).<br>2) Affiche pour comprendre quels genres obtiennent en moyenne les meilleures notes. |

---

## **Remarques Finales**
- Chaque « bout de code » est présenté dans l’ordre d’exécution.  
- Les explications soulignent les **opérations principales**, la **raison d’être** de la commande et **l’impact** sur les données.  
- Les champs d’application incluent : **jointures**, **agrégations**, **conversions de dates**, **filtrages** et **création de vues temporaires** pour utiliser **Spark SQL**.  
- Les DataFrames finaux (`df_movies_with_ratings`, `df_rating`, `df_rating_per_genre`, etc.) peuvent servir à bâtir **des analyses plus poussées** ou **des visualisations**.  

Fin de la table récapitulative.
