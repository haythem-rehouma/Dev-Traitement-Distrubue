# **Correction de l‚ÄôExamen Mi-Session (Partie 2) ‚Äì Syst√®mes Distribu√©s et Big Data**  


# **QUESTION 1 ‚Äì Architecture de Syst√®mes Distribu√©s**  
### **Th√©or√®me CAP et compromis dans les bases de donn√©es distribu√©es**  

Le **th√©or√®me CAP**, formul√© par Eric Brewer, affirme qu‚Äôun syst√®me distribu√© ne peut garantir simultan√©ment **trois propri√©t√©s** :  
1. **Coh√©rence (Consistency, C)** : Tous les n≈ìuds du syst√®me voient les m√™mes donn√©es √† un instant donn√©.  
2. **Disponibilit√© (Availability, A)** : Chaque requ√™te re√ßoit une r√©ponse, m√™me en cas de panne de certains n≈ìuds.  
3. **Tol√©rance au Partage (Partition Tolerance, P)** : Le syst√®me continue √† fonctionner m√™me si la communication entre certains n≈ìuds est interrompue.  

Un syst√®me distribu√© ne peut garantir **au maximum que deux** de ces propri√©t√©s en m√™me temps. Cela entra√Æne des **compromis CAP**, qui influencent la conception des bases de donn√©es distribu√©es :  

| **Type de base de donn√©es** | **Compromis** | **Exemple** |
|-----------------------------|--------------|-------------|
| **CP (Coh√©rence + Tol√©rance au Partage)** | Sacrifie la disponibilit√©. | **HBase, MongoDB (mode strict), Redis (mode cluster)** |
| **AP (Disponibilit√© + Tol√©rance au Partage)** | Sacrifie la coh√©rence stricte. | **Cassandra, DynamoDB, CouchDB** |
| **CA (Coh√©rence + Disponibilit√©)** | Impossible dans un r√©seau partitionn√©. | **Syst√®mes centralis√©s (ex: PostgreSQL sur un seul serveur)** |

### **Exemple concret : Apache Cassandra (AP)**
Apache Cassandra est con√ßu pour des applications n√©cessitant **une disponibilit√© √©lev√©e et une tol√©rance aux pannes r√©seau**. Il privil√©gie la **disponibilit√©** en acceptant que les donn√©es puissent √™tre temporairement inconsistantes entre les n≈ìuds. La coh√©rence est ensuite assur√©e via un m√©canisme de **r√©plication asynchrone et d'anti-entropy** (r√©conciliation des donn√©es).  



# **QUESTION 2 ‚Äì Microservices et Communication**  
### **Avantages et d√©fis de l‚Äôarchitecture microservices**  

L‚Äôarchitecture microservices d√©compose une application en **services ind√©pendants** qui communiquent via des API.  

‚úÖ **Avantages :**  
- **Scalabilit√©** : Chaque service peut √™tre dimensionn√© ind√©pendamment.  
- **D√©ploiement rapide** : Mise √† jour d‚Äôun service sans impacter les autres.  
- **Flexibilit√© technologique** : Chaque service peut utiliser un langage ou un framework diff√©rent.  
- **R√©silience** : Une panne d‚Äôun service n‚Äôentra√Æne pas l‚Äôarr√™t total du syst√®me.  

‚ùå **Inconv√©nients :**  
- **Complexit√©** : Gestion accrue des interconnexions et des versions.  
- **Latence r√©seau** : Chaque appel inter-service ajoute un d√©lai.  
- **Monitoring difficile** : Multiplication des logs et des erreurs √† surveiller.  
- **Gestion des transactions** : Difficile d‚Äôassurer une coh√©rence ACID sur plusieurs services.  

### **Moyens de communication entre microservices**  

Les microservices peuvent communiquer via :  
1. **API REST (HTTP/HTTPS)** ‚Äì Simple, mais forte latence.  
2. **gRPC (Remote Procedure Call)** ‚Äì Rapide et optimis√© pour la performance.  
3. **Message Brokers (Kafka, RabbitMQ, AWS SQS)** ‚Äì Utilis√© pour une architecture event-driven.  
4. **GraphQL** ‚Äì Permet d‚Äôinterroger plusieurs services via une seule requ√™te.  

### **Strat√©gies pour g√©rer les √©checs de communication**  

1. **Timeouts et Retries** : Limiter le temps d‚Äôattente et retenter en cas d‚Äô√©chec.  
2. **Circuit Breaker (ex: Resilience4j, Hystrix)** : Stopper les appels vers un service en panne.  
3. **Fallbacks (Plan B)** : Fournir une r√©ponse de secours en cas d‚Äô√©chec.  
4. **Eventual Consistency** : Synchronisation asynchrone des services avec des √©v√©nements.  



# **QUESTION 3 ‚Äì Solutions de Stockage et de Traitement en Environnements Distribu√©s**  
### **(A) Stockage distribu√©**  

Le **stockage par blocs** d√©coupe les donn√©es en fragments distribu√©s sur plusieurs serveurs, assurant **tol√©rance aux pannes et haute disponibilit√©**.  

**Exemples de syst√®mes de stockage distribu√©s :**  

| **Syst√®me** | **Avantages** | **Inconv√©nients** |
|------------|-------------|-----------------|
| **HDFS (Hadoop Distributed File System)** | R√©plication des donn√©es, tol√©rant aux pannes | Faible performance en acc√®s al√©atoire |
| **Ceph** | Acc√®s objet, bloc et fichier, √©volutivit√© automatique | Configuration complexe |

---

### **(B) Traitement des donn√©es distribu√©es**  

**Comparaison de frameworks :**  

| **Framework** | **Architecture** | **Avantages** | **Inconv√©nients** |
|--------------|----------------|--------------|----------------|
| **Hadoop MapReduce** | Bas√© sur fichiers (HDFS) | Fiable pour gros volumes | Latence √©lev√©e |
| **Apache Spark** | In-memory (RDD) | Tr√®s rapide, support du streaming | Gourmand en RAM |



# **QUESTION 4 ‚Äì √âcosyst√®me Big Data**  
### **MapReduce vs DAG (Directed Acyclic Graph)**  

üìå **MapReduce**  
- Ex√©cute les t√¢ches de mani√®re **s√©quentielle** (map ‚Üí shuffle ‚Üí reduce).  
- Stocke les donn√©es interm√©diaires sur disque, ce qui ralentit le traitement.  
- **Exemple** : **Hadoop MapReduce** pour du traitement batch.  

üìå **DAG (Graphe Acyclique Dirig√©)**  
- D√©finit un pipeline optimis√© o√π **les t√¢ches peuvent s‚Äôex√©cuter en parall√®le**.  
- Les donn√©es interm√©diaires sont **trait√©es en m√©moire** (plus rapide).  
- **Exemple** : **Apache Spark**, qui optimise l‚Äôordre d‚Äôex√©cution des t√¢ches.  

| **Crit√®re** | **MapReduce** | **DAG (Spark, Flink)** |
|------------|--------------|----------------|
| **Latence** | √âlev√©e (stockage disque) | Faible (in-memory) |
| **Flexibilit√©** | Rigide | Adaptable |
| **Efficacit√©** | Bonne sur gros fichiers | Excellente sur flux rapides |

‚úÖ **Conclusion** : DAG est plus rapide et efficace pour des **analyses interactives** et du **streaming**. MapReduce reste pertinent pour des **traitements batch massifs** sur disque.  



# **QUESTION 5 ‚Äì Syst√®mes de Stockage Distribu√©s**  
### **Caract√©ristiques d‚Äôun syst√®me de fichiers distribu√© fiable**  

1. **R√©plication des donn√©es** : Chaque fichier est copi√© sur plusieurs n≈ìuds.  
2. **Tol√©rance aux pannes** : Fonctionne malgr√© la perte de certains n≈ìuds.  
3. **Consistance des m√©tadonn√©es** : Emp√™che les incoh√©rences.  
4. **√âvolutivit√©** : Ajout dynamique de serveurs sans perturber le syst√®me.  

### **Impact de la r√©plication et du partitionnement**  

üìå **R√©plication**  
- Sauvegarde plusieurs copies des donn√©es pour √©viter la perte.  
- Exemples : HDFS stocke chaque bloc **en 3 copies**.  

üìå **Partitionnement**  
- Divise les donn√©es entre plusieurs serveurs pour am√©liorer la rapidit√©.  
- Exemples : Cassandra r√©partit les donn√©es avec une fonction de hash.  

**Exemple concret : Google File System (GFS)**  
- G√®re des **fichiers massifs** via des "chunks" r√©pliqu√©s.  
- Les n≈ìuds "Master" et "Chunkserver" assurent **scalabilit√© et tol√©rance aux pannes**.  



# **Conclusion**  

- Cette correction d√©taill√©e vous fournit des explications p√©dagogiques, des comparaisons pr√©cises et des exemples concrets pour chaque question. 
- Pour exceller, il est essentiel de structurer vos r√©ponses avec **des d√©finitions claires, des exemples illustratifs et des tableaux comparatifs**.  
