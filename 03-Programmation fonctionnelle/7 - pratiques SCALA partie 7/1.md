# **Examen 1 : Analyse de la Programmation Fonctionnelle en Scala pour le Big Data**  

**Objectif :** Cet examen ne vise pas Ã  coder mais Ã  **analyser et interprÃ©ter** les concepts abordÃ©s en **Big Data avec Scala et Apache Spark**.  

**Instructions :** Pour chaque question, **expliquez votre raisonnement** en analysant le code fourni et en rÃ©pondant aux questions posÃ©es.  

---

# **Partie 1 : DÃ©finition d'une `case class` pour reprÃ©senter des logs de serveur**  

### **Question 1 : CrÃ©ation de la `case class` `LogEntry`**  

```plaintext
DÃ©finir une case class LogEntry avec les attributs indiquÃ©s  
CrÃ©er une instance de LogEntry et l'afficher  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
LogEntry(2024-03-06T12:00:00, 192.168.1.1, /home, 200)
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
case class LogEntry(timestamp: String, ip: String, url: String, statusCode: Int)

val log = LogEntry("2024-03-06T12:00:00", "192.168.1.1", "/home", 200)
println(log)
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  
1. **Pourquoi utilise-t-on une `case class` au lieu d'une `class` normale dans ce contexte ?**  

Les `case class` en Scala offrent plusieurs avantages par rapport aux classes normales. Elles sont **immutables par dÃ©faut**, ce qui signifie que leurs valeurs ne peuvent pas Ãªtre modifiÃ©es aprÃ¨s leur crÃ©ation, ce qui garantit l'intÃ©gritÃ© des donnÃ©es dans un environnement distribuÃ© comme Spark. Elles gÃ©nÃ¨rent Ã©galement **automatiquement les mÃ©thodes `equals`, `hashCode` et `toString`**, facilitant la comparaison et la manipulation des objets. De plus, elles supportent le **pattern matching**, ce qui est utile pour manipuler les donnÃ©es de maniÃ¨re expressive et lisible.  

2. **Quels sont les avantages d'une `case class` en Big Data, notamment avec Spark ?**  

Dans Spark, les `case class` sont utilisÃ©es pour dÃ©finir des **schÃ©mas explicites** pour les `Dataset` et `DataFrame`. Cela permet Ã  Spark d'infÃ©rer le schÃ©ma automatiquement et de **bÃ©nÃ©ficier des optimisations Catalyst**, ce qui amÃ©liore la vitesse de traitement. Contrairement aux `RDD` qui traitent des tuples anonymes, les `case class` permettent un **typage fort**, rÃ©duisant les erreurs Ã  l'exÃ©cution et amÃ©liorant la lisibilitÃ© du code.  

3. **Quels problÃ¨mes pourrait-on rencontrer si on ne dÃ©finissait pas de `case class` et utilisait une structure gÃ©nÃ©rique comme un `Tuple` ou un `Map` ?**  

Si nous utilisions des `Tuple` ou des `Map`, le code deviendrait moins lisible et plus sujet aux erreurs. Par exemple, au lieu d'accÃ©der aux attributs par leur nom (`logEntry.timestamp`), il faudrait utiliser des indices (`tuple._1`), ce qui rend le code plus difficile Ã  comprendre et Ã  maintenir. De plus, Spark ne pourrait pas optimiser les transformations aussi efficacement, car il ne pourrait pas **infÃ©rer un schÃ©ma structurÃ©** comme avec une `case class`.  

---

# **Partie 2 : Chargement des logs en RDD et transformation**  

### **Question 2 : Charger un RDD de logs depuis un fichier**  

```plaintext
Lire un fichier de logs contenant des lignes au format : timestamp,ip,url,statusCode  
Convertir chaque ligne en un objet LogEntry  
Afficher les 5 premiÃ¨res lignes du RDD  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
Array(LogEntry(2024-03-06T12:00:00,192.168.1.1,/home,200), ...)
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder.appName("LogProcessing").master("local").getOrCreate()
val sc = spark.sparkContext

val logsRDD = sc.textFile("logs.csv")
  .map(_.split(","))
  .filter(_.length == 4)
  .map(cols => LogEntry(cols(0), cols(1), cols(2), cols(3).toInt))

logsRDD.take(5).foreach(println)
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  
1. **Pourquoi Spark utilise-t-il des RDD pour le traitement de fichiers volumineux ?**  

Les `RDD` (Resilient Distributed Datasets) sont la **structure de base** de Spark pour le traitement distribuÃ©. Ils permettent de **rÃ©partir automatiquement** les donnÃ©es sur plusieurs nÅ“uds et assurent la **tolÃ©rance aux pannes** en conservant un graphe d'exÃ©cution permettant de **recalculer les donnÃ©es perdues**. Les `RDD` supportent Ã©galement des **transformations paresseuses**, c'est-Ã -dire qu'aucun calcul n'est effectuÃ© tant qu'une action (comme `collect()` ou `count()`) n'est dÃ©clenchÃ©e. Cela permet d'optimiser le traitement des fichiers volumineux en **minimisant le nombre de passes sur les donnÃ©es**.  

2. **Quels avantages apporte le passage de donnÃ©es brutes (`String`) Ã  des objets `LogEntry` en termes de performance et de lisibilitÃ© ?**  

Lorsque les donnÃ©es restent sous forme de `String`, chaque transformation doit effectuer une **conversion explicite**, ce qui **augmente la charge de calcul** et rend le code plus difficile Ã  maintenir. En transformant directement les donnÃ©es en objets `LogEntry`, nous bÃ©nÃ©ficions dâ€™un **typage fort**, ce qui permet **d'Ã©viter des erreurs de conversion** et de rendre le code **plus clair et plus expressif**. De plus, cela permet Ã  Spark dâ€™appliquer des **optimisations basÃ©es sur le schÃ©ma** lorsquâ€™on convertit lâ€™`RDD` en `DataFrame`.  

3. **Dans un environnement de production, quel format de fichier serait prÃ©fÃ©rable (CSV, Parquet, JSON) et pourquoi ?**  

Le format **Parquet** est gÃ©nÃ©ralement prÃ©fÃ©rÃ© en production, car il est **colonne-orientÃ©**, ce qui permet dâ€™accÃ©lÃ©rer les requÃªtes en ne lisant que les colonnes nÃ©cessaires. Il offre Ã©galement une **compression efficace**, rÃ©duisant lâ€™espace de stockage et **accÃ©lÃ©rant la lecture** des fichiers. Contrairement au **CSV**, qui est ligne par ligne et non optimisÃ© pour la lecture partielle, Parquet est particuliÃ¨rement **adaptÃ© aux traitements analytiques**. Le **JSON**, quant Ã  lui, est plus lisible mais **consomme plus dâ€™espace** et **nÃ©cessite plus de puissance de calcul** pour Ãªtre parsÃ©.  





---

# **Partie 3 : Filtrage des erreurs 500 dans les logs**  

### **Question 3 : Filtrer les logs oÃ¹ `statusCode == 500`**  

```plaintext
Filtrer les logs pour ne garder que ceux avec un statusCode == 500  
Afficher le nombre d'erreurs 500  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
Nombre d'erreurs 500 : 42
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
val erreurs500 = logsRDD.filter(_.statusCode == 500)
val nombreErreurs500 = erreurs500.count()
println(s"Nombre d'erreurs 500 : $nombreErreurs500")
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  

1. **Pourquoi le filtrage des erreurs 500 est-il essentiel dans l'analyse des logs serveurs ?**  

Le code HTTP `500` indique une **erreur interne du serveur**, ce qui signifie qu'une requÃªte a Ã©chouÃ© **non pas Ã  cause du client**, mais Ã  cause d'un **problÃ¨me de traitement sur le serveur**. Ces erreurs peuvent rÃ©sulter de **problÃ¨mes logiciels**, d'une **base de donnÃ©es surchargÃ©e**, ou d'un **bug dans l'application**. Filtrer et analyser ces erreurs permet dâ€™**identifier rapidement les dysfonctionnements**, dâ€™**optimiser la stabilitÃ© du serveur**, et de **rÃ©duire le temps d'arrÃªt** des services.  

2. **Si nous devions surveiller les erreurs en temps rÃ©el, quel type dâ€™architecture (batch vs streaming) recommanderiez-vous et pourquoi ?**  

Une **architecture en streaming** serait recommandÃ©e pour une **dÃ©tection immÃ©diate** des erreurs et une **rÃ©action rapide**. Par exemple, **Apache Kafka** pourrait Ãªtre utilisÃ© pour **collecter les logs en temps rÃ©el**, tandis que **Spark Streaming** permettrait de **traiter ces donnÃ©es en continu** et dâ€™**envoyer des alertes** dÃ¨s qu'un certain seuil d'erreurs `500` est atteint.  

Ã€ lâ€™inverse, un **traitement batch** serait plus adaptÃ© pour **une analyse historique**, permettant de comprendre **les tendances et l'Ã©volution des erreurs** sur une longue pÃ©riode.  

3. **Comment pourrait-on optimiser ce filtrage pour des donnÃ©es massives dÃ©passant plusieurs tÃ©raoctets ?**  

Dans un contexte **Big Data**, plusieurs stratÃ©gies peuvent Ãªtre mises en place pour accÃ©lÃ©rer ce filtrage :  

- **Partitionnement des donnÃ©es** : Stocker les logs par **date** ou par **code HTTP** pour ne traiter que les fichiers pertinents plutÃ´t que lâ€™ensemble des logs.  
- **Indexation des fichiers** : Utiliser des formats comme **Parquet** qui permettent de filtrer les donnÃ©es directement au niveau du stockage sans charger lâ€™ensemble en mÃ©moire.  
- **Filtrage avant stockage** : Appliquer une **prÃ©modÃ©lisation** des logs en attribuant une **catÃ©gorie dâ€™erreur** pour ne rÃ©cupÃ©rer que celles qui sont critiques.  
- **Stockage en mÃ©moire avec cache()** : Si le filtrage est souvent rÃ©pÃ©tÃ©, stocker les rÃ©sultats intermÃ©diaires en mÃ©moire avec `cache()` pour Ã©viter de recharger les donnÃ©es Ã  chaque exÃ©cution.  

En combinant ces approches, on peut considÃ©rablement **rÃ©duire le temps de traitement** et **optimiser lâ€™analyse des erreurs** sur de trÃ¨s gros volumes de logs.  




----

# **Partie 4 : Compter les accÃ¨s par URL (`reduceByKey`)**  

### **Question 4 : Nombre d'accÃ¨s par URL**  

```plaintext
Transformer le RDD pour obtenir des tuples (url, 1)  
Utiliser reduceByKey pour compter le nombre d'accÃ¨s par URL  
Afficher les 5 URLs les plus visitÃ©es  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
/home -> 5000 accÃ¨s  
/about -> 3500 accÃ¨s  
/contact -> 1200 accÃ¨s  
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
val urlCounts = logsRDD
  .map(log => (log.url, 1)) // Transformer chaque log en un tuple (url, 1)
  .reduceByKey(_ + _) // AgrÃ©ger les accÃ¨s par URL

val top5Urls = urlCounts
  .sortBy(_._2, ascending = false) // Trier par nombre d'accÃ¨s, en ordre dÃ©croissant
  .take(5) // RÃ©cupÃ©rer les 5 URLs les plus visitÃ©es

top5Urls.foreach { case (url, count) => println(s"$url -> $count accÃ¨s") }
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  

1. **Pourquoi `reduceByKey` est-il plus efficace que `groupByKey` dans Spark ?**  

`reduceByKey` est **plus efficace** que `groupByKey` car il **rÃ©duit localement les valeurs** avant de les **envoyer sur le rÃ©seau**, ce qui diminue **le volume de donnÃ©es transfÃ©rÃ©es** entre les nÅ“uds.  

Dans `groupByKey`, toutes les valeurs associÃ©es Ã  une clÃ© sont envoyÃ©es Ã  un mÃªme nÅ“ud, ce qui **peut crÃ©er un goulet dâ€™Ã©tranglement mÃ©moire** si une clÃ© possÃ¨de trop de valeurs. En revanche, `reduceByKey` applique lâ€™agrÃ©gation **en amont**, permettant une meilleure **rÃ©partition des charges** et **une exÃ©cution plus rapide** sur des datasets volumineux.  

2. **Si nous devions calculer les URL les plus consultÃ©es sur une pÃ©riode dâ€™un an, quelles techniques de partitionnement ou d'indexation proposeriez-vous ?**  

Pour optimiser la requÃªte sur **une annÃ©e complÃ¨te de logs**, il serait intÃ©ressant de :  

- **Partitionner les donnÃ©es par date** (ex: annÃ©e/mois/jour) pour limiter la quantitÃ© de donnÃ©es traitÃ©es Ã  chaque requÃªte.  
- **Stocker les logs sous format Parquet** avec une organisation en colonnes, permettant des lectures sÃ©lectives trÃ¨s rapides.  
- **Utiliser des index secondaires** (comme **Z-Order Indexing** sur Databricks) pour accÃ©lÃ©rer la recherche des URL populaires sans nÃ©cessiter de lecture complÃ¨te du dataset.  
- **Mettre en cache les rÃ©sultats intermÃ©diaires** si cette requÃªte est exÃ©cutÃ©e frÃ©quemment.  

Avec ces stratÃ©gies, le calcul des URLs les plus visitÃ©es pourrait Ãªtre **optimisÃ© et exÃ©cutÃ© de maniÃ¨re incrÃ©mentale** au lieu de tout recalculer Ã  chaque fois.  

3. **Comment Spark gÃ¨re-t-il la distribution des calculs lorsquâ€™on utilise `reduceByKey` ?**  

Lorsqu'on utilise `reduceByKey`, Spark applique une **exÃ©cution distribuÃ©e** en plusieurs Ã©tapes :  

1. **Phase de mappage** : Spark transforme chaque ligne en une paire clÃ©-valeur `(url, 1)`.  
2. **Phase de rÃ©duction locale** : Chaque partition effectue une agrÃ©gation intermÃ©diaire **localement** pour rÃ©duire la quantitÃ© de donnÃ©es envoyÃ©es sur le rÃ©seau.  
3. **Phase de shuffle et agrÃ©gation finale** : Les rÃ©sultats intermÃ©diaires sont envoyÃ©s aux nÅ“uds responsables des diffÃ©rentes clÃ©s (urls), qui effectuent une **rÃ©duction finale** pour obtenir le nombre total dâ€™accÃ¨s par URL.  

Ce mÃ©canisme de **combinaison locale avant transmission** permet d'optimiser **l'utilisation du rÃ©seau** et d'assurer un traitement efficace sur des **datasets massifs**.  



---


# **Partie 5 : Transformation en `DataFrame`**  

### **Question 5 : Convertir les logs en `DataFrame`**  

```plaintext
Convertir le RDD de LogEntry en DataFrame  
Afficher le schÃ©ma du DataFrame  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
root
 |-- timestamp: string
 |-- ip: string
 |-- url: string
 |-- statusCode: integer
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder.appName("LogProcessing").master("local").getOrCreate()
import spark.implicits._

// Convertir le RDD en DataFrame
val logsDF = logsRDD.toDF()

// Afficher le schÃ©ma du DataFrame
logsDF.printSchema()
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  

1. **Pourquoi est-il prÃ©fÃ©rable d'utiliser des `DataFrame` plutÃ´t que des `RDD` en termes de performances ?**  

Les `DataFrame` sont **plus optimisÃ©s** que les `RDD` car ils bÃ©nÃ©ficient de **Catalyst Optimizer**, un moteur d'optimisation interne de Spark qui :  

- **RÃ©organise les opÃ©rations** pour minimiser le nombre de scans des donnÃ©es.  
- **Fusionne les transformations** pour Ã©viter les recalculs inutiles.  
- **Utilise Tungsten** pour optimiser lâ€™exÃ©cution en mÃ©moire et tirer parti des **instructions bas niveau du processeur**.  
- **Prend en charge des formats colonne-orientÃ©s** comme **Parquet**, qui permettent des lectures plus rapides que les RDD traditionnels.  

Contrairement aux RDD, qui sont **des collections dâ€™objets distribuÃ©es**, les `DataFrame` sont **structurÃ©s** et permettent dâ€™utiliser une syntaxe proche de SQL, facilitant lâ€™interaction avec les donnÃ©es.  

2. **Quelles optimisations Spark peut-il appliquer automatiquement sur un `DataFrame` mais pas sur un `RDD` ?**  

Spark applique plusieurs **optimisations automatiques** sur un `DataFrame` grÃ¢ce Ã  Catalyst Optimizer :  

- **Projection Pushdown** : Spark lit uniquement les colonnes nÃ©cessaires au lieu de charger lâ€™ensemble du dataset.  
- **Predicate Pushdown** : Les filtres (`WHERE` en SQL) sont appliquÃ©s **avant** de charger les donnÃ©es, rÃ©duisant ainsi le volume Ã  traiter.  
- **Rewriting des requÃªtes** : Spark peut rÃ©arranger certaines expressions SQL pour exÃ©cuter les calculs plus efficacement.  
- **Optimisation des jointures** : Catalyst choisit automatiquement lâ€™algorithme de jointure optimal (`Broadcast Join`, `Sort Merge Join`, etc.) en fonction de la taille des datasets.  

Ces optimisations permettent de **rÃ©duire la latence et dâ€™amÃ©liorer les performances** sans nÃ©cessiter dâ€™interventions manuelles.  

3. **Quels seraient les avantages d'utiliser un `Dataset` au lieu d'un `DataFrame` dans ce cas ?**  

Les `Dataset` offrent plusieurs avantages :  

- **Typage fort** : Contrairement aux `DataFrame`, qui sont typÃ©s dynamiquement (`Row`), un `Dataset[LogEntry]` conserve les types Scala, Ã©vitant les erreurs de conversion et amÃ©liorant la sÃ©curitÃ© Ã  la compilation.  
- **Meilleure expressivitÃ©** : Avec un `Dataset`, on peut utiliser des **opÃ©rations fonctionnelles Scala** (`map`, `filter`, etc.) avec un typage strict, ce qui amÃ©liore la lisibilitÃ© du code.  
- **Combinaison des avantages de RDD et DataFrame** : Il offre Ã  la fois **les optimisations Catalyst** et la **puissance des transformations fonctionnelles**.  

Cependant, les `Dataset` sont lÃ©gÃ¨rement plus **lents que les DataFrame** dans les cas oÃ¹ Spark peut utiliser des **opÃ©rations SQL pures**, car la sÃ©rialisation dâ€™objets Scala peut introduire une lÃ©gÃ¨re surcharge.  




---

# **Partie 6 : Filtrer les logs avec les `DataFrame API`**  

### **Question 6 : SÃ©lectionner les logs dâ€™erreur avec DataFrame API**  

```plaintext
Filtrer le DataFrame pour ne garder que les erreurs  
SÃ©lectionner les colonnes timestamp, ip et statusCode  
Afficher les 5 premiÃ¨res erreurs  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```
timestamp       | ip             | statusCode
-------------------------------------------------
2024-03-06T12:00:00 | 192.168.1.1 | 500
2024-03-06T12:05:00 | 192.168.1.2 | 404
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
import org.apache.spark.sql.functions._

// Filtrer uniquement les erreurs (statusCode >= 400)
val erreursDF = logsDF.filter(col("statusCode") >= 400)
  .select("timestamp", "ip", "statusCode")

// Afficher les 5 premiÃ¨res erreurs
erreursDF.show(5)
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  

1. **Pourquoi est-il prÃ©fÃ©rable d'utiliser `DataFrame API` au lieu de manipuler les `RDD` directement pour filtrer les erreurs ?**  

Lâ€™utilisation des `DataFrame API` offre plusieurs **avantages par rapport aux RDD** :  

- **Optimisation automatique** : Spark applique **Catalyst Optimizer**, qui optimise la requÃªte en **rÃ©organisant les filtres**, **rÃ©duisant le volume des donnÃ©es** traitÃ©es et **optimisant lâ€™accÃ¨s aux fichiers**.  
- **Utilisation de code SQL-like** : La syntaxe est **plus concise** et **plus lisible** quâ€™avec les transformations RDD (`map`, `filter`, etc.).  
- **Meilleure gestion de la mÃ©moire** : Les `DataFrame` utilisent des **structures de donnÃ©es optimisÃ©es** en mÃ©moire (Tungsten), ce qui rÃ©duit **l'empreinte mÃ©moire** et amÃ©liore **lâ€™exÃ©cution parallÃ¨le** sur le cluster Spark.  

Ainsi, filtrer les erreurs directement sur un `DataFrame` est **plus efficace et plus performant** que dâ€™utiliser un `RDD`.  

2. **Pourquoi le filtrage est-il plus efficace lorsquâ€™il est appliquÃ© en amont du traitement des donnÃ©es ?**  

Le **Predicate Pushdown** est une technique dâ€™optimisation utilisÃ©e par Spark qui consiste Ã  **appliquer les filtres aussi tÃ´t que possible** dans lâ€™exÃ©cution du plan de requÃªte.  

- Si le dataset est stockÃ© dans un **format colonne-orientÃ© comme Parquet**, Spark peut **charger uniquement les colonnes et lignes nÃ©cessaires** au lieu de lire lâ€™intÃ©gralitÃ© du fichier.  
- En filtrant en amont, on **rÃ©duit le volume de donnÃ©es traitÃ©es** par les transformations suivantes (`select`, `groupBy`, etc.), amÃ©liorant ainsi **les performances globales**.  
- Moins de donnÃ©es en mÃ©moire signifie **moins de consommation de ressources**, ce qui rÃ©duit les risques dâ€™**OOM (Out of Memory)** sur les grands datasets.  

3. **Comment pourrait-on optimiser davantage ce filtrage sur un cluster Spark exÃ©cutant des requÃªtes en continu ?**  

Si le filtrage est exÃ©cutÃ© en **streaming ou en batch rÃ©current**, plusieurs optimisations peuvent Ãªtre mises en place :  

- **Indexation et partitionnement** : Stocker les logs **partitionnÃ©s par date et statusCode**, pour que Spark ne charge que les partitions concernÃ©es.  
- **Utilisation de cache ou persist()** : Si les erreurs doivent Ãªtre analysÃ©es plusieurs fois, stocker le `DataFrame` en mÃ©moire avec `.cache()` ou `.persist(StorageLevel.MEMORY_AND_DISK)`.  
- **Filtrage prÃ©coce avec Delta Lake** : En utilisant **Delta Lake**, on peut exÃ©cuter **des filtres transactionnels** et ne rÃ©cupÃ©rer que les nouvelles erreurs depuis la derniÃ¨re exÃ©cution.  
- **Optimisation via le format de stockage** : PrÃ©fÃ©rer **Parquet** avec **Z-Ordering sur la colonne statusCode** pour accÃ©lÃ©rer les requÃªtes filtrÃ©es.  

En combinant ces approches, on **rÃ©duit le temps de traitement** et on **amÃ©liore la scalabilitÃ©** du pipeline d'analyse des erreurs.  



---

# **Partie 7 : CrÃ©ation dâ€™un `Dataset` pour un typage fort**  

### **Question 7 : Utiliser `Dataset[LogEntry]` au lieu de `DataFrame`**  

```plaintext
Convertir le DataFrame en Dataset[LogEntry]  
Afficher les 5 premiers logs du Dataset  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
Dataset[LogEntry]
+-------------------+--------------+--------+-----------+
| timestamp        | ip           | url    | statusCode |
+-------------------+--------------+--------+-----------+
| 2024-03-06T12:00 | 192.168.1.1  | /home  | 200       |
+-------------------+--------------+--------+-----------+
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
// Import nÃ©cessaire pour les Datasets
import spark.implicits._

// Convertir le DataFrame en Dataset[LogEntry]
val logsDS = logsDF.as[LogEntry]

// Afficher les 5 premiers logs du Dataset
logsDS.show(5)
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  

1. **Quelle est la principale diffÃ©rence entre un `DataFrame` et un `Dataset` ?**  

Les `DataFrame` et `Dataset` ont une structure **similaire**, mais la **diffÃ©rence principale** rÃ©side dans le **typage** :  

- Un **`DataFrame`** est une collection de **`Row` dynamiques**, oÃ¹ les types de colonnes ne sont **connus quâ€™Ã  lâ€™exÃ©cution**. Il est optimisÃ© pour les **opÃ©rations SQL et les requÃªtes analytiques**.  
- Un **`Dataset[LogEntry]`** est **typÃ© statiquement**, ce qui signifie que les erreurs sont **dÃ©tectÃ©es Ã  la compilation**. Il permet dâ€™utiliser **les opÃ©rations fonctionnelles Scala (`map`, `filter`) tout en conservant les optimisations Catalyst**.  

En rÃ©sumÃ©, **le Dataset combine la puissance de la programmation fonctionnelle avec les optimisations de Spark SQL**.  

2. **Pourquoi choisir un `Dataset[LogEntry]` dans ce cas plutÃ´t quâ€™un `DataFrame` ?**  

Le choix dâ€™un `Dataset[LogEntry]` est prÃ©fÃ©rable lorsque :  

- **On veut un typage fort** : En utilisant une `case class`, Spark garantit que **les opÃ©rations sont sÃ»res** et que les erreurs de type sont dÃ©tectÃ©es **avant lâ€™exÃ©cution**.  
- **On effectue beaucoup d'opÃ©rations fonctionnelles** (`map`, `flatMap`, `filter`) : Le `Dataset` offre **une meilleure expressivitÃ©** que les `DataFrame`.  
- **On veut Ã©viter des erreurs de conversion** : Dans un `DataFrame`, une erreur peut survenir si lâ€™on accÃ¨de Ã  une colonne avec un mauvais type (`df("statusCode").as[Int]` peut Ã©chouer Ã  lâ€™exÃ©cution).  

Cependant, **un `DataFrame` est plus rapide** lorsquâ€™on exÃ©cute uniquement des requÃªtes **SQL-like**, car Spark peut optimiser davantage les plans dâ€™exÃ©cution sans se soucier du typage Scala.  

3. **Quels seraient les cas oÃ¹ un `DataFrame` serait plus appropriÃ© quâ€™un `Dataset` ?**  

Lâ€™utilisation dâ€™un `DataFrame` est **prÃ©fÃ©rable** dans les situations suivantes :  

- **Traitement de trÃ¨s grands volumes de donnÃ©es** : Un `DataFrame` bÃ©nÃ©ficie de meilleures **optimisations Catalyst**, alors quâ€™un `Dataset` doit manipuler des **objets Scala**, ce qui **augmente la sÃ©rialisation** et peut ralentir les traitements.  
- **RequÃªtes SQL complexes** : Si l'on exÃ©cute beaucoup de **jointures et dâ€™agrÃ©gations**, un `DataFrame` est plus efficace car Spark optimise **lâ€™exÃ©cution en colonne-orientÃ©**.  
- **InteropÃ©rabilitÃ© avec d'autres outils** : Les `DataFrame` sont compatibles avec **PySpark, R et dâ€™autres langages**, alors que les `Dataset` sont spÃ©cifiques Ã  **Scala et Java**.  

Dans ce cas prÃ©cis, **si nous devons effectuer des analyses avancÃ©es sur les logs**, un `Dataset[LogEntry]` est un bon choix car il offre **la lisibilitÃ© du code avec la sÃ©curitÃ© du typage fort**.  





---

# **Partie 8 : Aggregation avec `groupBy` et `count`**  

### **Question 8 : Compter les accÃ¨s par IP**  

```plaintext
Grouper les logs par IP  
Compter le nombre d'accÃ¨s par IP  
Afficher les 5 IPs les plus actives  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
192.168.1.1 -> 1000 accÃ¨s  
192.168.1.2 -> 850 accÃ¨s  
192.168.1.3 -> 720 accÃ¨s  
192.168.1.4 -> 500 accÃ¨s  
192.168.1.5 -> 350 accÃ¨s  
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
import org.apache.spark.sql.functions._

// Grouper les logs par IP et compter les accÃ¨s
val accessByIP = logsDF
  .groupBy("ip")
  .count()
  .orderBy(desc("count"))

// Afficher les 5 IPs les plus actives
accessByIP.show(5)
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  

1. **Pourquoi utiliser `groupBy` et `count` dans ce cas, et quelles optimisations Spark applique-t-il en interne ?**  

L'agrÃ©gation avec `groupBy("ip").count()` permet de **compter efficacement le nombre d'accÃ¨s par IP** sans nÃ©cessiter de calculs complexes.  

**Optimisations appliquÃ©es par Spark :**  

- **Aggregation Pushdown** : Spark exÃ©cute **les filtres et regroupements directement au niveau du stockage** (ex. Parquet, ORC), ce qui rÃ©duit la quantitÃ© de donnÃ©es Ã  lire.  
- **Hash Aggregation** : Au lieu dâ€™un tri coÃ»teux, Spark utilise un **hash map distribuÃ©**, qui est plus rapide pour agrÃ©ger des milliers de valeurs uniques.  
- **Combinaison locale** : Avant d'envoyer les rÃ©sultats sur le rÃ©seau, Spark **prÃ©-agrÃ¨ge les donnÃ©es dans chaque partition**, limitant le volume de shuffle (transfert rÃ©seau).  

GrÃ¢ce Ã  ces optimisations, Spark peut **analyser des milliards de logs en quelques secondes**.  

2. **Pourquoi faut-il trier les rÃ©sultats (`orderBy(desc("count"))`) avant dâ€™afficher les IPs les plus actives ?**  

Sans tri, les rÃ©sultats seraient affichÃ©s **dans un ordre alÃ©atoire**, ce qui ne permettrait pas d'identifier immÃ©diatement les IPs les plus actives.  

L'ajout de `.orderBy(desc("count"))` permet de :  

- **Prioriser les IPs gÃ©nÃ©rant le plus de trafic**, facilitant lâ€™analyse des comportements anormaux.  
- **Optimiser les dÃ©cisions de sÃ©curitÃ©**, comme identifier des attaques DDoS ou bloquer des IPs suspectes.  
- **Faciliter la visualisation des donnÃ©es**, en mettant en Ã©vidence les IPs Ã  surveiller en prioritÃ©.  

Dans un **environnement distribuÃ©**, Spark optimise le tri en utilisant **un algorithme de tri distribuÃ©**, garantissant une exÃ©cution efficace mÃªme sur des **datasets massifs**.  

3. **Comment gÃ©rer ce type dâ€™agrÃ©gation si les logs sont stockÃ©s sur plusieurs jours ou mois ?**  

Si lâ€™on doit **analyser les accÃ¨s IP sur plusieurs jours/mois**, il est prÃ©fÃ©rable de :  

- **Partitionner les logs par date (`logsDF.write.partitionBy("date")`)** pour Ã©viter de charger toutes les donnÃ©es Ã  chaque requÃªte.  
- **Utiliser des index secondaires (`Z-Order`, `Bloom Filters`)** pour accÃ©lÃ©rer la recherche des IPs les plus actives sur une pÃ©riode donnÃ©e.  
- **Mettre en cache les rÃ©sultats (`cache()` ou `persist()`)** si lâ€™analyse des IPs les plus actives est rÃ©alisÃ©e frÃ©quemment.  
- **Utiliser Delta Lake avec Time Travel** : Cela permet dâ€™analyser **les logs Ã  diffÃ©rentes pÃ©riodes** sans requÃªtes coÃ»teuses.  

GrÃ¢ce Ã  ces techniques, il est possible d'effectuer des **analyses de tendances sur plusieurs mois tout en gardant un temps de requÃªte optimal**.  





---

# **Partie 9 : Gestion des erreurs avec `Try` et `Option`**  

### **Question 9 : SÃ©curiser la transformation de donnÃ©es avec `Option`**  

```plaintext
Utiliser Option pour Ã©viter les erreurs lors du parsing des logs  
Retourner None si une ligne est mal formatÃ©e  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
Certaines lignes de logs mal formÃ©es sont ignorÃ©es.
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
import scala.util.Try

// Fonction pour parser une ligne de log en LogEntry
def parseLogLine(line: String): Option[LogEntry] = {
  val cols = line.split(",")
  if (cols.length == 4) {
    Try(LogEntry(cols(0), cols(1), cols(2), cols(3).toInt)).toOption
  } else {
    None
  }
}

// Appliquer la transformation en sÃ©curisant le parsing
val parsedLogsRDD = logsRDD.flatMap(parseLogLine)

// VÃ©rifier le nombre de logs valides aprÃ¨s filtrage
println(s"Nombre de logs valides : ${parsedLogsRDD.count()}")
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  

1. **Pourquoi utiliser `Option` et `Try` pour le parsing des logs ?**  

Lâ€™utilisation combinÃ©e de `Option` et `Try` permet de **sÃ©curiser le parsing** en Ã©vitant les erreurs qui pourraient interrompre lâ€™exÃ©cution du programme.  

- **`Option`** est utilisÃ© pour gÃ©rer la possibilitÃ© quâ€™une ligne de log soit mal formÃ©e et Ã©viter les erreurs fatales en **retournant `None` au lieu d'une exception**.  
- **`Try`** permet de capturer les erreurs lors de la conversion (`.toInt` peut Ã©chouer si `statusCode` nâ€™est pas un entier) et de retourner un `None` proprement.  

Avec cette approche, **les logs valides sont traitÃ©s normalement** tandis que **les logs mal formÃ©s sont ignorÃ©s sans perturber lâ€™exÃ©cution globale**.  

2. **Quels problÃ¨mes pourraient survenir si lâ€™on nâ€™utilise pas `Option` pour filtrer les logs mal formÃ©s ?**  

Si on nâ€™utilise pas `Option`, plusieurs problÃ¨mes peuvent apparaÃ®tre :  

- **Crash de lâ€™application** : Une erreur de conversion (`cols(3).toInt`) pourrait interrompre Spark et stopper le traitement de lâ€™ensemble des logs.  
- **Perte de scalabilitÃ©** : Un seul log mal formatÃ© pourrait empÃªcher lâ€™analyse de plusieurs tÃ©raoctets de donnÃ©es.  
- **DifficultÃ© de debugging** : Les erreurs pourraient Ãªtre propagÃ©es sans Ãªtre gÃ©rÃ©es proprement, compliquant le diagnostic des logs erronÃ©s.  

Avec `Option`, on garantit que **les logs erronÃ©s sont automatiquement filtrÃ©s**, tout en **prÃ©servant lâ€™analyse des logs valides**.  

3. **Comment pourrait-on gÃ©rer ces erreurs Ã  grande Ã©chelle sans perdre dâ€™informations sur les logs mal formÃ©s ?**  

Si nous voulons **analyser aussi les erreurs** et non juste les ignorer, nous pouvons utiliser **une approche plus avancÃ©e** :  

- **Diviser les logs en deux catÃ©gories** :  
  - Les logs valides sont stockÃ©s dans un `RDD[LogEntry]`.  
  - Les logs mal formÃ©s sont enregistrÃ©s sÃ©parÃ©ment avec un message dâ€™erreur.  

- **Stocker les erreurs dans un fichier dÃ©diÃ©** :  
  - Au lieu de perdre ces logs, on pourrait **les stocker dans un DataFrame contenant `ligne_originale` + `erreur_detectÃ©e`**.  

- **CrÃ©er une alerte en cas de pics dâ€™erreurs** :  
  - Si un nombre trop Ã©levÃ© de logs mal formÃ©s est dÃ©tectÃ©, une **alerte** peut Ãªtre dÃ©clenchÃ©e pour avertir lâ€™Ã©quipe dâ€™ingÃ©nierie.  

Voici une alternative qui permettrait d'archiver les erreurs tout en filtrant les logs valides :  

```scala
val (validLogsRDD, invalidLogsRDD) = logsRDD.map(line => (line, parseLogLine(line)))
  .partition { case (_, parsed) => parsed.isDefined }

val errorsDF = invalidLogsRDD.map { case (line, _) => (line, "Parsing error") }.toDF("raw_log", "error_reason")
errorsDF.write.format("parquet").save("errors_logs/")
```

Avec cette approche, nous avons **un contrÃ´le total sur les erreurs**, et nous pouvons analyser **les logs erronÃ©s pour amÃ©liorer la qualitÃ© des donnÃ©es en amont**.  



---

# **Partie 10 : Optimisation avec `cache()` et `persist()`**  

### **Question 10 : AccÃ©lÃ©rer les requÃªtes avec `cache()`**  

```plaintext
Ajouter un cache() ou persist() sur le DataFrame des erreurs  
```

```scala
...........................................................
...........................................................
...........................................................
```

ğŸ“Œ **ğŸ’¡ Output attendu :**  

```scala
Les requÃªtes sont accÃ©lÃ©rÃ©es grÃ¢ce au cache.
```

#### **ğŸ’¬ RÃ©ponse du code :**  

```scala
import org.apache.spark.storage.StorageLevel

// Appliquer un cache sur le DataFrame des erreurs pour Ã©viter de recalculer les transformations
val erreursDF = logsDF.filter(col("statusCode") >= 400)
  .select("timestamp", "ip", "statusCode")
  .cache() // Mise en cache en mÃ©moire

// VÃ©rifier si le cache est bien utilisÃ©
println(s"Nombre d'erreurs 400 et plus : ${erreursDF.count()}")

// Alternative avec persist() pour stockage sur disque et mÃ©moire
val erreursPersistedDF = logsDF.filter(col("statusCode") >= 400)
  .select("timestamp", "ip", "statusCode")
  .persist(StorageLevel.MEMORY_AND_DISK) // Sauvegarde en mÃ©moire et sur disque en cas de surcharge
```

#### **ğŸ’¬ Analyse et rÃ©flexion**  

1. **Quelle est la diffÃ©rence entre `cache()` et `persist()` en Spark ?**  

- **`cache()`** : Stocke les donnÃ©es **uniquement en mÃ©moire**, ce qui permet dâ€™Ã©viter de **recalculer les transformations** Ã  chaque exÃ©cution.  
- **`persist(level)`** : Permet de choisir **le niveau de stockage**, par exemple :  
  - `MEMORY_ONLY` : Stocke les donnÃ©es uniquement en mÃ©moire (Ã©quivalent Ã  `cache()`).  
  - `MEMORY_AND_DISK` : Stocke les donnÃ©es en mÃ©moire, et si lâ€™espace manque, les Ã©crit sur disque.  
  - `DISK_ONLY` : Garde les donnÃ©es uniquement sur disque, utile si la mÃ©moire est limitÃ©e.  

Si les donnÃ©es **tiennent en mÃ©moire**, `cache()` est plus rapide. Si elles sont **trop volumineuses**, `persist(StorageLevel.MEMORY_AND_DISK)` est prÃ©fÃ©rable.  

2. **Pourquoi utiliser `cache()` amÃ©liore-t-il les performances, notamment dans les requÃªtes rÃ©pÃ©tÃ©es ?**  

Sans `cache()`, Spark **recalcule Ã  chaque fois toutes les transformations** (`filter()`, `select()`, etc.) dÃ¨s qu'une nouvelle action (`count()`, `show()`) est dÃ©clenchÃ©e.  

Avec `cache()` :  
- Les **donnÃ©es transformÃ©es** sont conservÃ©es en mÃ©moire, **Ã©vitant un recalcul coÃ»teux**.  
- Les **requÃªtes rÃ©pÃ©tÃ©es** sur les mÃªmes donnÃ©es sont **beaucoup plus rapides**.  
- Cela **rÃ©duit lâ€™utilisation du CPU et des ressources du cluster**, car Spark **nâ€™a plus besoin de relire et retraiter les donnÃ©es**.  

âš  **Attention** : `cache()` doit Ãªtre utilisÃ© uniquement si **les donnÃ©es sont utilisÃ©es plusieurs fois**. Si les donnÃ©es ne sont utilisÃ©es qu'une seule fois, `cache()` peut **alourdir inutilement la mÃ©moire**.  

3. **Dans quels cas le `cache()` pourrait-il Ãªtre une mauvaise pratique, et quelle alternative recommanderiez-vous ?**  

Le `cache()` peut **dÃ©grader les performances** si :  
- **Les donnÃ©es sont trop volumineuses** pour tenir en mÃ©moire, ce qui **entraÃ®ne des Ã©victions frÃ©quentes** et des recalculs involontaires.  
- **Les transformations ne sont utilisÃ©es qu'une seule fois**, rendant le cache inutile.  
- **Dâ€™autres processus Spark utilisent la mÃ©moire**, ce qui peut **entraÃ®ner des erreurs OutOfMemory (OOM)**.  

ğŸ“Œ **Alternatives recommandÃ©es :**  
- **Utiliser `persist(StorageLevel.MEMORY_AND_DISK)`** pour Ã©viter la perte de cache en cas de manque de mÃ©moire.  
- **MatÃ©rialiser les rÃ©sultats en Ã©crivant dans un fichier temporaire** (ex. format Parquet sur HDFS ou S3).  
- **Utiliser Delta Lake** qui permet de conserver **des snapshots optimisÃ©s** de DataFrames.  

Exemple dâ€™une alternative avec Ã©criture de DataFrame sur disque pour Ã©viter lâ€™usage intensif de la mÃ©moire :  

```scala
erreursDF.write.mode("overwrite").parquet("hdfs://path/to/temp/errors")
```

Cela permet de **stocker et rÃ©cupÃ©rer les erreurs** sans surcharger la mÃ©moire de Spark.  

---

Avec cette derniÃ¨re optimisation, nous avons vu **comment Spark gÃ¨re efficacement la mÃ©moire et les performances** en utilisant **cache et persist**.  

ğŸ’¡ **DerniÃ¨re question gÃ©nÃ©rale :**  
âœ… **Si vous deviez amÃ©liorer cet algorithme pour le rendre encore plus performant et scalable, quelles stratÃ©gies proposeriez-vous ?**  

---

ğŸ¯ **Cet examen vous a permis d'explorer** :  
- **Les bases du traitement Big Data avec Scala et Spark**.  
- **Lâ€™importance des structures optimisÃ©es (`case class`, RDD, DataFrame, Dataset)`**.  
- **Les bonnes pratiques pour analyser les logs et gÃ©rer les erreurs**.  
- **Les techniques avancÃ©es pour optimiser les performances en environnement distribuÃ©**.  

ğŸ’­ **PrÃªt Ã  passer Ã  un cas d'utilisation rÃ©el ?**

---

# Annexe - **Cas d'utilisation rÃ©el : Analyse des logs d'un site web en production**  

### **ProblÃ©matique :**  
Une entreprise gÃ©rant un site Ã  **fort trafic** souhaite **analyser les logs de ses serveurs web** afin de :  
- Identifier les **erreurs frÃ©quentes** (ex : erreurs 500, requÃªtes non trouvÃ©es).  
- DÃ©tecter les **IPs suspectes** gÃ©nÃ©rant un trafic anormal.  
- Trouver les **pages les plus visitÃ©es** et celles posant problÃ¨me.  

Les logs sont stockÃ©s sur un **cluster HDFS** et sont mis Ã  jour **en continu**.  

---

## **Objectifs de l'analyse :**  
Vous devez rÃ©pondre aux questions suivantes **en analysant les donnÃ©es avec Spark** :  

### **1. Identification des erreurs les plus frÃ©quentes**  
ğŸ“Œ **Question :**  
- Comment dÃ©tecter les **erreurs HTTP les plus frÃ©quentes** et en visualiser l'Ã©volution dans le temps ?  

ğŸ’¡ **Piste de rÃ©flexion :**  
- Faut-il utiliser un **batch processing** ou un **streaming processing** ? Pourquoi ?  
- Comment **structurer les donnÃ©es** pour obtenir des tendances temporelles efficaces ?  

---

### **2. DÃ©tection des IPs suspectes**  
ğŸ“Œ **Question :**  
- Comment dÃ©tecter **des adresses IP suspectes** ayant un comportement anormal (ex : attaques DDoS) ?  

ğŸ’¡ **Piste de rÃ©flexion :**  
- Quels indicateurs pourraient signaler une attaque (ex : **nombre de requÃªtes par seconde**, **rÃ©partition des codes dâ€™erreurs**) ?  
- Comment **partitionner les donnÃ©es** pour amÃ©liorer la dÃ©tection dâ€™anomalies ?  

---

### **3. Analyse des pages les plus visitÃ©es**  
ğŸ“Œ **Question :**  
- Comment identifier **les pages qui gÃ©nÃ¨rent le plus de trafic** et celles qui posent problÃ¨me ?  

ğŸ’¡ **Piste de rÃ©flexion :**  
- Faut-il compter uniquement les accÃ¨s ou Ã©galement **le temps de rÃ©ponse des pages** ?  
- Comment visualiser ces informations efficacement (ex : **tableau de bord avec Grafana, tableau Parquet optimisÃ©**).  

---

## **Approche technique recommandÃ©e :**  
Voici les grandes Ã©tapes dâ€™implÃ©mentation avec **Spark et Scala** :  

1. **Lecture des logs depuis HDFS ou S3** :  
   - StockÃ©s en **format Parquet (optimisÃ©)** ou en **CSV/JSON brut**.  
   - Chargement via `spark.read.parquet()` ou `spark.read.csv()`.  

2. **Transformation et filtrage des donnÃ©es** :  
   - Convertir en **DataFrame / Dataset** pour bÃ©nÃ©ficier des **optimisations Catalyst**.  
   - Nettoyage des logs erronÃ©s via **`Option` et `Try`**.  

3. **Analyse des tendances et anomalies** :  
   - **AgrÃ©gation temporelle** (`groupBy("timestamp").count()`) pour visualiser lâ€™Ã©volution des erreurs.  
   - **DÃ©tection dâ€™IP suspectes** avec des **requÃªtes Spark SQL** et un **seuil dâ€™alerte dynamique**.  

4. **Optimisation et persistance des rÃ©sultats** :  
   - Stockage des rÃ©sultats **en mÃ©moire (`cache()` ou `persist()`)** si requÃªtes frÃ©quentes.  
   - Sauvegarde des rÃ©sultats optimisÃ©s en **Parquet** sur HDFS ou **Delta Lake**.  

---

## **Livrables attendus :**  
ğŸ¯ **Dans un contexte professionnel, vous devriez livrer :**  
âœ” **Un script Scala Spark** exÃ©cutant l'analyse et gÃ©nÃ©rant des tableaux de bord.  
âœ” **Un rapport technique** expliquant les rÃ©sultats et les axes d'amÃ©lioration.  
âœ” **Une proposition dâ€™optimisation de lâ€™architecture** pour un meilleur traitement des logs en temps rÃ©el.  

---

### **DerniÃ¨re question dâ€™analyse :**  
âœ… **Si vous deviez mettre en place un systÃ¨me dâ€™analyse de logs scalable sur plusieurs annÃ©es, comment structureriez-vous votre pipeline de donnÃ©es ?**  

ğŸ’¡ **RÃ©flexion attendue :**  
- **Comment stocker les logs efficacement (Partitionnement, Delta Lake, Indexation) ?**  
- **Quelle solution utiliser pour accÃ©lÃ©rer les requÃªtes sur des gros volumes (Spark, Presto, Apache Druid) ?**  
- **Comment intÃ©grer un systÃ¨me de monitoring pour les erreurs en temps rÃ©el ?**  
